{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ddfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "\n",
    "# --- Core Libraries ---\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the workspace root to Python path\n",
    "workspace_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if workspace_root not in sys.path:\n",
    "    sys.path.insert(0, workspace_root)\n",
    "# Add workspace root to path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "# Cell 1: Setup and Imports (Partial)\n",
    "\n",
    "import time # <--- ADD THIS LINE\n",
    "# ... rest of your Cell 1 code ...\n",
    "\n",
    "# --- Database & System Libraries ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Import Custom DB Connector ---\n",
    "from src.common.database.MQSDBConnector import MQSDBConnector\n",
    "\n",
    "# --- Notebook Configuration ---\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Configure logging for better debugging and tracing.\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported and MQSDBConnector ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading and Preparation (with Yearly Loop)\n",
    "\n",
    "# --- 1. Load API Key ---\n",
    "# (Assumes 'FMP_API_KEY' is in your .env file)\n",
    "fmp_api_key = os.getenv(\"FMP_API_KEY\")\n",
    "\n",
    "if not fmp_api_key:\n",
    "    logging.error(\"FMP_API_KEY not found in environment variables...\")\n",
    "    # raise ValueError(\"FMP_API_KEY not found.\")\n",
    "\n",
    "# --- 2. Define Data Fetching Function (Yearly Loop Version) ---\n",
    "\n",
    "def _get_market_data(tickers_list: list, lookback_days: int, api_key: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches daily (end-of-day) historical data from the FMP API.\n",
    "    \n",
    "    ATTEMPT: This version loops by year to try and bypass API limits.\n",
    "    WARNING: This will likely NOT work if the API key plan itself\n",
    "             restricts historical data access to only 1 year.\n",
    "    \n",
    "    :param tickers_list: List of stock symbols\n",
    "    :param lookback_days: How many days of data to fetch\n",
    "    :param api_key: The FMP API key\n",
    "    :return: Pandas DataFrame of historical records\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Prepare Parameters ---\n",
    "    if not tickers_list:\n",
    "        logging.warning(\"No tickers provided. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    tickers_str = \",\".join(tickers_list)\n",
    "    \n",
    "    # Calculate start and end years for the loop\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=lookback_days)\n",
    "    \n",
    "    start_year = start_date.year\n",
    "    end_year = end_date.year # This will be the current year\n",
    "\n",
    "    all_dfs = [] # List to hold DataFrame for each year\n",
    "    \n",
    "    logging.info(f\"Starting yearly data fetch from {start_year} to {end_year} for {tickers_str}...\")\n",
    "\n",
    "    # --- 2. Loop Through Each Year ---\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        \n",
    "        # Define the date range for this specific year\n",
    "        from_date_loop = f\"{year}-01-01\"\n",
    "        to_date_loop = f\"{year}-12-31\"\n",
    "        \n",
    "        # Override 'to_date' if we are in the current year\n",
    "        if year == end_year:\n",
    "            to_date_loop = end_date.date().isoformat()\n",
    "\n",
    "        logging.info(f\"--- Fetching data for year: {year} ---\")\n",
    "\n",
    "        url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{tickers_str}\"\n",
    "        params = {\"from\": from_date_loop, \"to\": to_date_loop, \"apikey\": api_key}\n",
    "\n",
    "        # --- Make API Request (per year) ---\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status() \n",
    "            data = response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"API request failed for {year}: {e}\")\n",
    "            continue # Skip to next year\n",
    "\n",
    "        # --- Parse FMP Response (for this year) ---\n",
    "        historical_data = []\n",
    "        if isinstance(data, dict) and 'historical' in data:\n",
    "            logging.info(f\"Processing single ticker response for {year}\")\n",
    "            for record in data['historical']:\n",
    "                record['symbol'] = tickers_str\n",
    "                historical_data.append(record)\n",
    "                \n",
    "        elif isinstance(data, dict) and 'historicalStockList' in data:\n",
    "            logging.info(f\"Processing multi-ticker response for {year}\")\n",
    "            for stock in data['historicalStockList']:\n",
    "                t_symbol = stock['symbol']\n",
    "                for record in stock['historical']:\n",
    "                    record['symbol'] = t_symbol\n",
    "                    historical_data.append(record)\n",
    "        else:\n",
    "            logging.warning(f\"No valid data found for {year}.\")\n",
    "            \n",
    "        if historical_data:\n",
    "            logging.info(f\"Successfully parsed {len(historical_data)} data points for {year}.\")\n",
    "            all_dfs.append(pd.DataFrame(historical_data))\n",
    "        else:\n",
    "            logging.warning(f\"No data returned from API for {year}.\")\n",
    "            \n",
    "        # **CRUCIAL**: Wait for a moment to avoid API rate limits\n",
    "        # (e.g., 0.5 - 1 second between calls)\n",
    "        time.sleep(0.5) \n",
    "\n",
    "    # --- 3. Combine All DataFrames ---\n",
    "    if not all_dfs:\n",
    "        logging.warning(\"No data fetched for any year. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df = pd.concat(all_dfs)\n",
    "\n",
    "    # --- 4. Convert to DataFrame and Clean (Run once on the final DF) ---\n",
    "    df.rename(columns={\n",
    "        'date': 'timestamp',\n",
    "        'close': 'close_price',\n",
    "        'open': 'open_price',\n",
    "        'high': 'high_price',\n",
    "        'low': 'low_price',\n",
    "        'symbol': 'ticker'\n",
    "    }, inplace=True)\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    num_cols = [\n",
    "        'open_price', 'high_price', 'low_price', 'close_price', 'adjClose',\n",
    "        'volume', 'unadjustedVolume', 'change', 'changePercent', 'vwap'\n",
    "    ]\n",
    "    for col in num_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df.dropna(subset=['timestamp', 'ticker', 'close_price'], inplace=True)\n",
    "    df.sort_values(['timestamp', 'ticker'], inplace=True)\n",
    "    \n",
    "    logging.info(f\"Successfully processed {len(df)} TOTAL data points from all years.\")\n",
    "    return df\n",
    "\n",
    "# --- 3. Execute Data Fetching ---\n",
    "\n",
    "tickers = ['AAPL', 'MSFT', 'GOOG'] \n",
    "model_lookback_days = 365 * 10 # ~20 years\n",
    "\n",
    "if fmp_api_key:\n",
    "    all_data = _get_market_data(tickers, model_lookback_days, fmp_api_key)\n",
    "    print(f\"Loaded {len(all_data)} rows of end-of-day data.\")\n",
    "    print(\"Data head:\")\n",
    "    print(all_data.head())\n",
    "    print(\"\\nData tail:\")\n",
    "    print(all_data.tail())\n",
    "else:\n",
    "    print(\"Skipping data load because FMP_API_KEY is not set.\")\n",
    "    all_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "####cell 3: Data Preparation for Modeling\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Ensure all_data is loaded from the previous cell.\n",
    "# This simple check assumes 'all_data' is a global DataFrame.\n",
    "if 'all_data' not in globals() or all_data.empty:\n",
    "    logging.error(\"all_data DataFrame not found or is empty. Please run Cell 2 first.\")\n",
    "    # In a real notebook, you might 'raise' an error, but here we'll just log.\n",
    "    \n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineers predictive features (X) and an outcome variable (Y)\n",
    "    from the raw market data.\n",
    "    \n",
    "    X variables (predictors):\n",
    "    - past_return_21d: 1-month (21 trading days) past return\n",
    "    - past_vol_21d: 1-month past volatility (std dev of daily returns)\n",
    "    - past_return_63d: 3-month (63 trading days) past return\n",
    "    - past_vol_63d: 3-month past volatility\n",
    "    - past_return_252d: 1-year (252 trading days) past return\n",
    "    \n",
    "    Y variable (outcome):\n",
    "    - target_return_21d: 1-month future return\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting feature engineering...\")\n",
    "    \n",
    "    # Work on a copy to avoid SettingWithCopyWarning\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Ensure data is sorted for time-series operations\n",
    "    data.sort_values(['ticker', 'timestamp'], inplace=True)\n",
    "    \n",
    "    # Calculate daily returns as the basis for volatility\n",
    "    daily_returns = data.groupby('ticker')['close_price'].pct_change()\n",
    "    \n",
    "    # --- Create X Variables (Predictors) ---\n",
    "    \n",
    "    # Group by ticker to apply rolling/shifting operations correctly\n",
    "    grouped = data.groupby('ticker')\n",
    "    \n",
    "    # 1. Past Returns (Momentum)\n",
    "    data['past_return_21d'] = grouped['close_price'].pct_change(21)\n",
    "    data['past_return_63d'] = grouped['close_price'].pct_change(63)\n",
    "    data['past_return_252d'] = grouped['close_price'].pct_change(252)\n",
    "\n",
    "    # 2. Past Volatility\n",
    "    # We use .transform() to align the rolling std dev with the original DataFrame index\n",
    "    data['past_vol_21d'] = daily_returns.groupby(data['ticker']).transform(lambda x: x.rolling(21).std())\n",
    "    data['past_vol_63d'] = daily_returns.groupby(data['ticker']).transform(lambda x: x.rolling(63).std())\n",
    "\n",
    "    # --- Create Y Variable (Target) ---\n",
    "    \n",
    "    # Calculate 21-day FUTURE return. We shift(-21) to pull future data back.\n",
    "    data['target_return_21d'] = grouped['close_price'].shift(-21) / data['close_price'] - 1\n",
    "    \n",
    "    # --- Clean Up and Return ---\n",
    "    \n",
    "    # Drop rows with NaNs created by rolling windows or target shifting\n",
    "    # We now have X and Y variables in the same row.\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    logging.info(f\"Feature engineering complete. {len(data)} rows remain after NaN removal.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# --- 1. Engineer Features ---\n",
    "processed_data = engineer_features(all_data)\n",
    "\n",
    "# --- 2. Define X and Y Column Names ---\n",
    "# These are our K=5 predictive variables\n",
    "FEATURE_COLS = [\n",
    "    'past_return_21d',\n",
    "    'past_vol_21d',\n",
    "    'past_return_63d',\n",
    "    'past_vol_63d',\n",
    "    'past_return_252d'\n",
    "]\n",
    "TARGET_COL = 'target_return_21d'\n",
    "\n",
    "# --- 3. Split into Training and Testing Sets ---\n",
    "# We use a date-based split for time-series data, as seen in the paper.\n",
    "# Train on 2005-2019, Test on 2020-2025.\n",
    "split_date = pd.to_datetime('2020-01-01')\n",
    "\n",
    "train_df = processed_data[processed_data['timestamp'] < split_date]\n",
    "test_df = processed_data[processed_data['timestamp'] >= split_date]\n",
    "\n",
    "# Separate X (predictors) and y (target)\n",
    "X_train = train_df[FEATURE_COLS]\n",
    "y_train = train_df[TARGET_COL]\n",
    "\n",
    "X_test = test_df[FEATURE_COLS]\n",
    "y_test = test_df[TARGET_COL]\n",
    "\n",
    "print(\"Cell 3: Data Preparation Complete.\")\n",
    "print(f\"Total processed rows: {len(processed_data)}\")\n",
    "print(f\"Training set: {len(X_train)} rows (up to {split_date.date()})\")\n",
    "print(f\"Testing set: {len(X_test)} rows (from {split_date.date()})\")\n",
    "print(\"\\nPredictive Variables (X):\")\n",
    "print(FEATURE_COLS)\n",
    "print(\"\\nOutcome Variable (Y):\")\n",
    "print(TARGET_COL)\n",
    "print(\"\\nX_train head:\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cell 4: Pre-computation and Relevance Functions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# This cell assumes X_train exists from Cell 3\n",
    "if 'X_train' not in globals():\n",
    "    logging.error(\"X_train not found. Please run Cell 3 first.\")\n",
    "    # Create a dummy DataFrame to prevent crashes, though logic will be flawed\n",
    "    X_train = pd.DataFrame(np.random.rand(100, 5), \n",
    "                           columns=['past_return_21d', 'past_vol_21d', \n",
    "                                    'past_return_63d', 'past_vol_63d', \n",
    "                                    'past_return_252d'])\n",
    "\n",
    "# --- Cell 3 (Template): Pre-computation on Training Data ---\n",
    "\n",
    "def compute_training_statistics(X_train_df):\n",
    "  \"\"\"\n",
    "  Computes the mean vector and inverse covariance matrix\n",
    "  from the training data, as described in [cite: 149].\n",
    "  \n",
    "  These are the \"x-bar\" and \"Omega-inverse\" used for all\n",
    "  Mahalanobis distance calculations.\n",
    "  \"\"\"\n",
    "  logging.info(f\"Computing statistics on {len(X_train_df)} training rows...\")\n",
    "  \n",
    "  # 1. Calculate the mean vector (x-bar) [cite: 149]\n",
    "  # .values creates a 1D numpy array: (K,)\n",
    "  x_mean = X_train_df.mean().values\n",
    "  \n",
    "  # 2. Calculate the covariance matrix (Omega)\n",
    "  cov_matrix = X_train_df.cov().values\n",
    "  \n",
    "  # 3. Calculate the inverse covariance matrix (Omega-inverse) [cite: 149]\n",
    "  # This is the key component for Mahalanobis distance\n",
    "  try:\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "  except np.linalg.LinAlgError:\n",
    "    logging.error(\"Covariance matrix is singular. Cannot compute inverse.\")\n",
    "    # Add a small amount of \"jitter\" (ridge regression) to the diagonal\n",
    "    # This helps stabilize the matrix if variables are highly collinear\n",
    "    logging.warning(\"Adding small identity matrix 'jitter' to stabilize...\")\n",
    "    cov_matrix_reg = cov_matrix + np.eye(cov_matrix.shape[0]) * 1e-6\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix_reg)\n",
    "\n",
    "  logging.info(f\"Computed x_mean ({x_mean.shape}) and inv_cov_matrix ({inv_cov_matrix.shape}).\")\n",
    "  \n",
    "  return x_mean, inv_cov_matrix, X_train_df.columns.tolist()\n",
    "\n",
    "# --- Cell 4 (Template): Core Helper Functions (Relevance) ---\n",
    "\n",
    "def calculate_mahalanobis_distance(vec1, vec2, inv_cov_matrix):\n",
    "  \"\"\"\n",
    "  Calculates the *squared* Mahalanobis distance between two vectors.\n",
    "  (x_i - x_t) * Omega^-1 * (x_i - x_t)'\n",
    "  \n",
    "  Args:\n",
    "    vec1 (np.array): 1D array of shape (K,)\n",
    "    vec2 (np.array): 1D array of shape (K,)\n",
    "    inv_cov_matrix (np.array): 2D array of shape (K, K)\n",
    "    \n",
    "  Returns:\n",
    "    float: The squared Mahalanobis distance.\n",
    "  \"\"\"\n",
    "  # Ensure inputs are numpy arrays (e.g., from df.values)\n",
    "  diff = vec1 - vec2  # Shape (K,)\n",
    "  \n",
    "  # We use (diff @ inv_cov @ diff) which is equivalent to\n",
    "  # (diff @ inv_cov @ diff.T) for 1D arrays in numpy.\n",
    "  # This calculates: (1, K) @ (K, K) @ (K, 1) -> scalar\n",
    "  m_dist_sq = diff @ inv_cov_matrix @ diff\n",
    "  \n",
    "  return m_dist_sq\n",
    "\n",
    "def calculate_relevance(x_i, x_t, x_mean, inv_cov_matrix):\n",
    "  \"\"\"\n",
    "  Calculates the relevance of a past observation (x_i) to a\n",
    "  current prediction task (x_t), based on Equation 1 [cite: 141].\n",
    "  \n",
    "  Args:\n",
    "    x_i (np.array): A single past observation vector (shape K,)\n",
    "    x_t (np.array): The current prediction task vector (shape K,)\n",
    "    x_mean (np.array): The mean vector of training data (shape K,)\n",
    "    inv_cov_matrix (np.array): The inverse covariance matrix (shape K, K)\n",
    "    \n",
    "  Returns:\n",
    "    float: The total relevance score (r_it).\n",
    "  \"\"\"\n",
    "\n",
    "  # 1. Similarity (sim) component, Equation 2 [cite: 142]\n",
    "  # This is the distance between the past obs and current task\n",
    "  sim_component = calculate_mahalanobis_distance(x_i, x_t, inv_cov_matrix)\n",
    "  \n",
    "  # 2. Informativeness of past observation (info(x_i)), Equation 3 [cite: 143]\n",
    "  # This is the distance between the past obs and the \"average\" obs\n",
    "  info_i = calculate_mahalanobis_distance(x_i, x_mean, inv_cov_matrix)\n",
    "  \n",
    "  # 3. Informativeness of current task (info(x_t)), Equation 4 [cite: 144]\n",
    "  # This is the distance between the current task and the \"average\" obs\n",
    "  info_t = calculate_mahalanobis_distance(x_t, x_mean, inv_cov_matrix)\n",
    "  \n",
    "  # 4. Total Relevance (r_it), Equation 1 [cite: 141]\n",
    "  r_it = (-0.5 * sim_component) + 0.5 * (info_i + info_t)\n",
    "  \n",
    "  return r_it\n",
    "\n",
    "# --- Execute Pre-computation ---\n",
    "print(\"Cell 4: Running Pre-computation...\")\n",
    "\n",
    "# Compute and store the fundamental training statistics\n",
    "# These will be used for every relevance calculation\n",
    "full_train_stats = {}\n",
    "(\n",
    "    full_train_stats['x_mean'],\n",
    "    full_train_stats['inv_cov'],\n",
    "    full_train_stats['vars']\n",
    ") = compute_training_statistics(X_train)\n",
    "\n",
    "print(\"\\nRelevance helper functions defined.\")\n",
    "print(\"Training statistics (mean, inv_cov) computed and stored.\")\n",
    "print(f\"Variables used: {full_train_stats['vars']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16944366",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cell 5: Prediction and Fit Helper Functions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# This cell assumes X_train, y_train, and the calculate_relevance function\n",
    "# from Cell 4 are available in the global scope.\n",
    "if 'X_train' not in globals() or 'y_train' not in globals():\n",
    "    logging.error(\"X_train or y_train not found. Please run Cell 3 first.\")\n",
    "    # Define dummies\n",
    "    X_train = pd.DataFrame(np.random.rand(100, 5))\n",
    "    y_train = pd.Series(np.random.rand(100))\n",
    "if 'calculate_relevance' not in globals():\n",
    "    logging.error(\"calculate_relevance not found. Please run Cell 4 first.\")\n",
    "    def calculate_relevance(x_i, x_t, x_mean, inv_cov_matrix):\n",
    "        return np.random.rand()\n",
    "\n",
    "def get_relevance_for_task(x_t, X_train, x_mean, inv_cov):\n",
    "  \"\"\"\n",
    "  Calculates relevance scores for ALL past observations (x_i)\n",
    "  against a SINGLE current task (x_t).\n",
    "  \n",
    "  Args:\n",
    "    x_t (np.array): The current prediction task vector (K,)\n",
    "    X_train (pd.DataFrame): All past observations (N, K)\n",
    "    x_mean (np.array): Mean vector from training (K,)\n",
    "    inv_cov (np.array): Inverse covariance matrix (K, K)\n",
    "    \n",
    "  Returns:\n",
    "    pd.Series: Relevance scores, indexed by X_train.index\n",
    "  \"\"\"\n",
    "  # Use .apply(axis=1) to iterate through each row (x_i)\n",
    "  # and call calculate_relevance for each one.\n",
    "  # x_i.values passes the row as a numpy array, which is fast.\n",
    "  relevance_scores = X_train.apply(\n",
    "      lambda x_i: calculate_relevance(x_i.values, x_t, x_mean, inv_cov),\n",
    "      axis=1\n",
    "  )\n",
    "  return relevance_scores\n",
    "\n",
    "def calculate_prediction_weights(relevance_scores, r_threshold_quantile=0.0):\n",
    "  \"\"\"\n",
    "  Calculates observation weights (w_it) based on relevance\n",
    "  and a censoring threshold, per Equations 7-9.\n",
    "  \n",
    "  If r_threshold_quantile is 0.0, this function uses the\n",
    "  linear regression equivalent weights from Equation 6.\n",
    "  \n",
    "  Args:\n",
    "    relevance_scores (pd.Series): Series of relevance scores for all x_i\n",
    "    r_threshold_quantile (float): The quantile for censoring (e.g., 0.2, 0.5)\n",
    "    \n",
    "  Returns:\n",
    "    tuple: (weights (pd.Series), retained_mask (pd.Series))\n",
    "  \"\"\"\n",
    "  N = len(relevance_scores)\n",
    "  \n",
    "  # --- Handle edge cases ---\n",
    "  if N < 2:\n",
    "      logging.warning(\"Not enough observations to calculate weights.\")\n",
    "      return pd.Series(np.nan, index=relevance_scores.index), \\\n",
    "             pd.Series(False, index=relevance_scores.index)\n",
    "             \n",
    "  # --- Linear Regression Case (Eq 6) ---\n",
    "  # This is the default if no threshold is applied (quantile=0)\n",
    "  if r_threshold_quantile == 0.0:\n",
    "      weights = (1 / N) + (1 / (N - 1)) * relevance_scores\n",
    "      retained_mask = pd.Series(True, index=relevance_scores.index)\n",
    "      return weights, retained_mask\n",
    "\n",
    "  # --- Censored Case (Eq 7-9) ---\n",
    "  \n",
    "  # 1. Determine the relevance threshold value (r*)\n",
    "  r_star = relevance_scores.quantile(r_threshold_quantile)\n",
    "  \n",
    "  # 2. Identify retained (delta=1) and censored (delta=0) observations\n",
    "  retained_mask = relevance_scores >= r_star\n",
    "  \n",
    "  # 3. Calculate n, phi, r_sub_avg, etc.\n",
    "  n = retained_mask.sum()\n",
    "  \n",
    "  # Handle edge case where n is too small to calculate variance or (n-1)\n",
    "  if n < 2:\n",
    "      # Revert to linear regression weights (Eq 6)\n",
    "      logging.warning(f\"Quantile {r_threshold_quantile} resulted in n < 2. \"\n",
    "                      f\"Reverting to linear weights (Eq 6).\")\n",
    "      weights = (1 / N) + (1 / (N - 1)) * relevance_scores\n",
    "      retained_mask = pd.Series(True, index=relevance_scores.index)\n",
    "      return weights, retained_mask\n",
    "\n",
    "  phi = n / N\n",
    "  retained_scores = relevance_scores[retained_mask]\n",
    "  r_sub_avg = retained_scores.mean()\n",
    "\n",
    "  # 4. Calculate the scaling factor (lambda^2) (Eq 9)\n",
    "  # Note: (x**2).sum() / (n-1) is the variance without subtracting the mean,\n",
    "  # which matches the paper's definition of variance of relevance.\n",
    "  var_r_full_sum = (relevance_scores**2).sum()\n",
    "  var_r_retained_sum = (retained_scores**2).sum()\n",
    "\n",
    "  if (N - 1) == 0 or (n - 1) == 0 or var_r_retained_sum == 0:\n",
    "      lambda_sq = 1.0 # Avoid division by zero\n",
    "  else:\n",
    "      var_r_full = var_r_full_sum / (N - 1)\n",
    "      var_r_retained = var_r_retained_sum / (n - 1)\n",
    "      lambda_sq = var_r_full / var_r_retained\n",
    "\n",
    "  # 5. Calculate final weights (w_it_retained) for all i (Eq 7)\n",
    "  # `delta(r_it) * r_it` is just the scores where retained, 0 otherwise\n",
    "  delta_r_it = relevance_scores.where(retained_mask, 0.0)\n",
    "  \n",
    "  term1 = 1 / N\n",
    "  term2_multiplier = lambda_sq / (n - 1)\n",
    "  term3_base = (delta_r_it - phi * r_sub_avg)\n",
    "  \n",
    "  weights = term1 + term2_multiplier * term3_base\n",
    "  \n",
    "  return weights, retained_mask\n",
    "\n",
    "def calculate_fit(weights, outcomes):\n",
    "  \"\"\"\n",
    "  Calculates the Fit for a prediction task, which is the\n",
    "  squared correlation of relevance weights and outcomes, per Eq. 11.\n",
    "  \n",
    "  Args:\n",
    "    weights (pd.Series): The prediction weights (w_it)\n",
    "    outcomes (pd.Series): The past outcomes (y_i)\n",
    "    \n",
    "  Returns:\n",
    "    float: The fit score (rho^2)\n",
    "  \"\"\"\n",
    "  # Ensure indices align for correlation\n",
    "  weights_aligned, outcomes_aligned = weights.align(outcomes)\n",
    "  \n",
    "  if weights_aligned.std() == 0 or outcomes_aligned.std() == 0:\n",
    "      return 0.0 # Cannot correlate if one variable is constant\n",
    "\n",
    "  rho = np.corrcoef(weights_aligned, outcomes_aligned)[0, 1]\n",
    "  \n",
    "  if np.isnan(rho):\n",
    "      return 0.0 # Handle potential NaN from std dev=0 or other issues\n",
    "      \n",
    "  return rho**2\n",
    "\n",
    "def calculate_asymmetry(weights, outcomes, retained_mask):\n",
    "  \"\"\"\n",
    "  Calculates asymmetry per Equation 13.\n",
    "  \n",
    "  Args:\n",
    "    weights (pd.Series): The prediction weights (w_it)\n",
    "    outcomes (pd.Series): The past outcomes (y_i)\n",
    "    retained_mask (pd.Series): Boolean mask of retained samples\n",
    "  \n",
    "  Returns:\n",
    "    float: The asymmetry score\n",
    "  \"\"\"\n",
    "  # Align all data\n",
    "  # FIX: Chain the alignments. align can only handle two Series at a time.\n",
    "  # 1. Align weights and outcomes\n",
    "  weights_aligned, outcomes_aligned = weights.align(outcomes)\n",
    "  # 2. Align the mask to the *new* aligned index of weights\n",
    "  weights_aligned, retained_mask_aligned = weights_aligned.align(retained_mask)\n",
    "  # 3. Align the outcomes to the *final* aligned index as well\n",
    "  outcomes_aligned, retained_mask_aligned = outcomes_aligned.align(retained_mask_aligned)\n",
    "\n",
    "  # 1. Get weights and outcomes for retained subsample (w_t_plus)\n",
    "  # FIX: Use the new aligned variables\n",
    "  w_retained = weights_aligned[retained_mask_aligned]\n",
    "  y_retained = outcomes_aligned[retained_mask_aligned]\n",
    "  \n",
    "  # 2. Get weights and outcomes for censored subsample (w_t_minus)\n",
    "  # FIX: Use the new aligned variables\n",
    "  w_censored = weights_aligned[~retained_mask_aligned]\n",
    "  y_censored = outcomes_aligned[~retained_mask_aligned]\n",
    "\n",
    "  # 3. Calculate correlations, handling edge cases\n",
    "  rho_plus = 0.0\n",
    "  if len(w_retained) >= 2 and w_retained.std() > 0 and y_retained.std() > 0:\n",
    "      rho_plus = np.corrcoef(w_retained, y_retained)[0, 1]\n",
    "      if np.isnan(rho_plus): rho_plus = 0.0\n",
    "      \n",
    "  rho_minus = 0.0\n",
    "  if len(w_censored) >= 2 and w_censored.std() > 0 and y_censored.std() > 0:\n",
    "      rho_minus = np.corrcoef(w_censored, y_censored)[0, 1]\n",
    "      if np.isnan(rho_minus): rho_minus = 0.0\n",
    "\n",
    "  # 4. Return asymmetry (Eq 13)\n",
    "  return 0.5 * (rho_plus - rho_minus)**2\n",
    "\n",
    "def calculate_adjusted_fit(fit, asymmetry, K):\n",
    "  \"\"\"\n",
    "  Calculates adjusted fit per Equation 14.\n",
    "  K is the number of predictive variables in this calibration.\n",
    "  \n",
    "  Args:\n",
    "    fit (float): The fit score (from calculate_fit)\n",
    "    asymmetry (float): The asymmetry score (from calculate_asymmetry)\n",
    "    K (int): Number of variables used\n",
    "    \n",
    "  Returns:\n",
    "    float: The adjusted fit score\n",
    "  \"\"\"\n",
    "  return K * (fit + asymmetry)\n",
    "\n",
    "print(\"Cell 5: Prediction and Fit helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c75f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cell 6: Example Calculation for a Single Prediction Task\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# --- Setup: Ensure all previous cells are run ---\n",
    "# This cell needs:\n",
    "# - From Cell 3: X_train, y_train, X_test\n",
    "# - From Cell 4: full_train_stats (x_mean, inv_cov), calculate_relevance\n",
    "# - From Cell 5: get_relevance_for_task, calculate_prediction_weights,\n",
    "#                calculate_fit, calculate_asymmetry, calculate_adjusted_fit\n",
    "\n",
    "if 'X_test' not in globals() or X_test.empty:\n",
    "    logging.error(\"X_test not found. Please run Cells 3, 4, and 5 first.\")\n",
    "    # Create a dummy x_t to prevent crash\n",
    "    x_t = np.random.rand(5)\n",
    "    x_t_index = \"dummy_index\"\n",
    "else:\n",
    "    # --- 1. Select a Single Prediction Task (x_t) ---\n",
    "    # We'll pick the first row from our test set\n",
    "    x_t_series = X_test.iloc[0]\n",
    "    x_t = x_t_series.values\n",
    "    x_t_index = x_t_series.name\n",
    "    \n",
    "    print(f\"--- Running example calculation for task: {x_t_index} ---\")\n",
    "    print(f\"Task vector (x_t):\\n{x_t_series.to_string()}\\n\")\n",
    "\n",
    "# --- 2. Get Relevance Scores ---\n",
    "# This calculates relevance for all N training rows against our single x_t\n",
    "print(\"Calculating relevance scores...\")\n",
    "relevance_scores = get_relevance_for_task(\n",
    "    x_t,\n",
    "    X_train,\n",
    "    full_train_stats['x_mean'],\n",
    "    full_train_stats['inv_cov']\n",
    ")\n",
    "print(f\"Top 5 most relevant past observations:\\n{relevance_scores.nlargest(5)}\\n\")\n",
    "print(f\"Bottom 5 least relevant past observations:\\n{relevance_scores.nsmallest(5)}\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Run Calculation for Linear Case (Eq 6) ---\n",
    "print(\"--- Scenario 1: Linear Regression (Quantile = 0.0) ---\")\n",
    "K_vars = X_train.shape[1] # K = 5 variables\n",
    "thresh_lin = 0.0\n",
    "\n",
    "weights_lin, mask_lin = calculate_prediction_weights(\n",
    "    relevance_scores, \n",
    "    thresh_lin\n",
    ")\n",
    "\n",
    "fit_lin = calculate_fit(weights_lin, y_train)\n",
    "\n",
    "# Asymmetry is 0 for linear case (mask is all True)\n",
    "asym_lin = calculate_asymmetry(weights_lin, y_train, mask_lin)\n",
    "\n",
    "adj_fit_lin = calculate_adjusted_fit(fit_lin, asym_lin, K_vars)\n",
    "\n",
    "# The final prediction is the weighted average of past outcomes\n",
    "pred_lin = (weights_lin * y_train).sum()\n",
    "\n",
    "print(f\"  Weights (w_it) head:\\n{weights_lin.head().to_string()}\\n\")\n",
    "print(f\"  Prediction (y_t): {pred_lin:.6f}\")\n",
    "print(f\"  Fit (rho^2):      {fit_lin:.6f}\")\n",
    "print(f\"  Asymmetry:        {asym_lin:.6f}\")\n",
    "print(f\"  Adjusted Fit:     {adj_fit_lin:.6f}\")\n",
    "\n",
    "\n",
    "# --- 4. Run Calculation for Censored Case (Eq 7-9) ---\n",
    "print(\"\\n--- Scenario 2: Censored RBP (Quantile = 0.2) ---\")\n",
    "thresh_cen = 0.2 # Retain top 80% most relevant\n",
    "\n",
    "weights_cen, mask_cen = calculate_prediction_weights(\n",
    "    relevance_scores, \n",
    "    thresh_cen\n",
    ")\n",
    "\n",
    "fit_cen = calculate_fit(weights_cen, y_train)\n",
    "asym_cen = calculate_asymmetry(weights_cen, y_train, mask_cen)\n",
    "adj_fit_cen = calculate_adjusted_fit(fit_cen, asym_cen, K_vars)\n",
    "pred_cen = (weights_cen * y_train).sum()\n",
    "\n",
    "print(f\"  Relevance Threshold (r*): {relevance_scores.quantile(thresh_cen):.6f}\")\n",
    "print(f\"  Retained 'n' obs:       {mask_cen.sum()} / {len(mask_cen)}\")\n",
    "print(f\"  Weights (w_it) head:\\n{weights_cen.head().to_string()}\\n\")\n",
    "print(f\"  Prediction (y_t): {pred_cen:.6f}\")\n",
    "print(f\"  Fit (rho^2):      {fit_cen:.6f}\")\n",
    "print(f\"  Asymmetry:        {asym_cen:.6f}\")\n",
    "print(f\"  Adjusted Fit:     {adj_fit_cen:.6f}\")\n",
    "\n",
    "print(\"\\nCell 6: Example calculation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cell 7: Full Grid Processing for One Prediction Task\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import itertools # <-- NEW IMPORT for variable combinations\n",
    "\n",
    "# --- Setup: Ensure all previous cells are run ---\n",
    "# This cell needs:\n",
    "# - From Cell 3: X_train, y_train, X_test\n",
    "# - From Cell 4: compute_training_statistics, get_relevance_for_task\n",
    "# - From Cell 5: calculate_prediction_weights, calculate_fit,\n",
    "#                calculate_asymmetry, calculate_adjusted_fit\n",
    "\n",
    "if 'X_train' not in globals():\n",
    "    logging.error(\"X_train not found. Please run Cells 3, 4, and 5 first.\")\n",
    "    # Define dummies\n",
    "    X_train = pd.DataFrame(np.random.rand(100, 5), columns=[f'V{i}' for i in range(5)])\n",
    "    X_test = X_train.copy()\n",
    "    y_train = pd.Series(np.random.rand(100))\n",
    "\n",
    "# --- New Helper Function for Grid Columns ---\n",
    "\n",
    "def get_variable_combinations(all_columns):\n",
    "  \"\"\"\n",
    "  Generates all 2^K - 1 non-empty subsets of variables.\n",
    "  \n",
    "  Args:\n",
    "    all_columns (list): List of column names\n",
    "    \n",
    "  Returns:\n",
    "    list of tuples: Each tuple is a variable combination\n",
    "  \"\"\"\n",
    "  combinations = []\n",
    "  for k in range(1, len(all_columns) + 1):\n",
    "    # Get all combinations of length k\n",
    "    for combo in itertools.combinations(all_columns, k):\n",
    "      combinations.append(combo)\n",
    "  return combinations\n",
    "\n",
    "# --- Core Grid Processing Function ---\n",
    "\n",
    "def process_grid_for_one_task(x_t_series, X_train, y_train):\n",
    "  \"\"\"\n",
    "  Builds the entire prediction grid (Exhibit 1) for a\n",
    "  single prediction task (x_t_series).\n",
    "  \n",
    "  Args:\n",
    "    x_t_series (pd.Series): The current prediction task (K,)\n",
    "    X_train (pd.DataFrame): All past observations (N, K)\n",
    "    y_train (pd.Series): All past outcomes (N,)\n",
    "    \n",
    "  Returns:\n",
    "    pd.DataFrame: A DataFrame of [params, prediction, adj_fit]\n",
    "  \"\"\"\n",
    "  \n",
    "  # --- 1. Define the Grid ---\n",
    "  \n",
    "  # Columns: All 2^K - 1 combinations of variables\n",
    "  variable_combinations = get_variable_combinations(X_train.columns)\n",
    "  \n",
    "  # Rows: Relevance thresholds\n",
    "  relevance_thresholds = [0.0, 0.2, 0.5, 0.8]\n",
    "  \n",
    "  logging.info(f\"Processing grid for one task: \"\n",
    "               f\"{len(variable_combinations)} var combos x \"\n",
    "               f\"{len(relevance_thresholds)} thresholds = \"\n",
    "               f\"{len(variable_combinations) * len(relevance_thresholds)} cells.\")\n",
    "\n",
    "  # --- 2. Initialize Grid Results ---\n",
    "  grid_results = [] # Store (cell_params, prediction, adjusted_fit)\n",
    "  \n",
    "  # --- 3. Iterate through Grid Cells (theta) ---\n",
    "  \n",
    "  for var_combo_tuple in variable_combinations:\n",
    "    # Convert tuple to list for pandas indexing\n",
    "    var_combo = list(var_combo_tuple)\n",
    "    K = len(var_combo)\n",
    "    \n",
    "    # --- 3a. Setup for this Variable Subset ---\n",
    "    X_train_sub = X_train[var_combo]\n",
    "    x_t_sub = x_t_series[var_combo].values # .values to make it a np array\n",
    "    \n",
    "    # Re-compute stats for this *subset*\n",
    "    # This is critical: x_mean and inv_cov are specific to the var subset\n",
    "    x_mean_sub, inv_cov_sub, _ = compute_training_statistics(X_train_sub)\n",
    "    \n",
    "    # Get relevance scores for this task, using this subset\n",
    "    relevance_scores = get_relevance_for_task(\n",
    "        x_t_sub, X_train_sub, x_mean_sub, inv_cov_sub\n",
    "    )\n",
    "    \n",
    "    # Iterate through all relevance thresholds (rows)\n",
    "    for r_thresh in relevance_thresholds:\n",
    "      # --- 3b. Process this Cell (theta) ---\n",
    "      cell_params = {'vars': var_combo_tuple, 'thresh': r_thresh, 'K': K}\n",
    "      \n",
    "      # 1. Get prediction weights\n",
    "      weights, mask = calculate_prediction_weights(relevance_scores, r_thresh)\n",
    "      \n",
    "      # 2. Get cell prediction (y_hat_theta)\n",
    "      # Ensure indices align between weights and y_train before summing\n",
    "      y_hat_theta = (weights * y_train.align(weights)[0]).sum()\n",
    "      \n",
    "      # 3. Get cell reliability\n",
    "      fit = calculate_fit(weights, y_train)\n",
    "      asymmetry = calculate_asymmetry(weights, y_train, mask)\n",
    "      adj_fit = calculate_adjusted_fit(fit, asymmetry, K)\n",
    "      \n",
    "      # 4. Store cell results\n",
    "      grid_results.append({\n",
    "          'params': cell_params, \n",
    "          'prediction': y_hat_theta, \n",
    "          'adj_fit': adj_fit\n",
    "      })\n",
    "\n",
    "  return pd.DataFrame(grid_results)\n",
    "\n",
    "# --- Test the function with the first task ---\n",
    "print(\"Cell 7: Testing grid processing for one task...\")\n",
    "\n",
    "# Get the first task from the test set\n",
    "x_t_example = X_test.iloc[0]\n",
    "\n",
    "grid_for_task_0 = process_grid_for_one_task(x_t_example, X_train, y_train)\n",
    "\n",
    "print(\"\\nGrid results for one task (head):\")\n",
    "print(grid_for_task_0.head())\n",
    "\n",
    "print(\"\\nGrid results for one task (tail):\")\n",
    "print(grid_for_task_0.tail())\n",
    "\n",
    "print(f\"\\nTotal cells computed: {len(grid_for_task_0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcd12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cell 8: Final Prediction and RBI Calculation for One Task\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# --- Setup: Ensure all previous cells are run ---\n",
    "# This cell needs:\n",
    "# - From Cell 3: FEATURE_COLS\n",
    "# - From Cell 7: grid_for_task_0 (the DataFrame)\n",
    "\n",
    "if 'grid_for_task_0' not in globals():\n",
    "    logging.error(\"grid_for_task_0 not found. Please run Cell 7 first.\")\n",
    "    # Define dummies\n",
    "    grid_for_task_0 = pd.DataFrame({\n",
    "        'params': [{'vars': ('V1',), 'thresh': 0.0, 'K': 1}],\n",
    "        'prediction': [0.05],\n",
    "        'adj_fit': [0.1]\n",
    "    })\n",
    "if 'FEATURE_COLS' not in globals():\n",
    "    logging.error(\"FEATURE_COLS not found. Please run Cell 3 first.\")\n",
    "    FEATURE_COLS = ['V1']\n",
    "\n",
    "# --- Calculate Final Prediction ---\n",
    "\n",
    "def calculate_composite_prediction(grid_df):\n",
    "  \"\"\"\n",
    "  Forms the composite grid prediction (y_hat_grid)\n",
    "  per Equations 15 and 16.\n",
    "  \n",
    "  Args:\n",
    "    grid_df (pd.DataFrame): The output from process_grid_for_one_task\n",
    "    \n",
    "  Returns:\n",
    "    tuple: (y_hat_grid (float), psi_weights (pd.Series))\n",
    "  \"\"\"\n",
    "  # 1. Get all adjusted fits. Clip at 0 to ensure no negative weights.\n",
    "  adj_fits = grid_df['adj_fit'].clip(lower=0)\n",
    "  \n",
    "  # 2. Calculate reliability weights (psi_theta), Eq. 15\n",
    "  sum_adj_fits = adj_fits.sum()\n",
    "  \n",
    "  # Handle case where all reliability is zero\n",
    "  if sum_adj_fits == 0:\n",
    "      logging.warning(\"Sum of all adjusted fits is 0. Prediction is unreliable.\")\n",
    "      psi_weights = pd.Series(0.0, index=grid_df.index)\n",
    "      y_hat_grid = 0.0 # No reliable prediction\n",
    "  else:\n",
    "      psi_weights = adj_fits / sum_adj_fits\n",
    "  \n",
    "  # 3. Calculate composite prediction (y_hat_grid), Eq. 16\n",
    "  y_hat_grid = (psi_weights * grid_df['prediction']).sum()\n",
    "  \n",
    "  return y_hat_grid, psi_weights\n",
    "\n",
    "# --- Calculate RBI ---\n",
    "\n",
    "def calculate_rbi_for_task(grid_df, all_variables):\n",
    "  \"\"\"\n",
    "  Calculates RBI for every variable for a single task,\n",
    "  using the grid results per Equation 18.\n",
    "  \n",
    "  This uses the simplified, but mathematically equivalent,\n",
    "  \"average marginal contribution\" approach.\n",
    "  \n",
    "  Args:\n",
    "    grid_df (pd.DataFrame): The grid results for this task\n",
    "    all_variables (list): The list of all possible variable names\n",
    "    \n",
    "  Returns:\n",
    "    pd.Series: RBI scores, indexed by variable name\n",
    "  \"\"\"\n",
    "  rbi_scores = {}\n",
    "  \n",
    "  # We need to analyze the 'params' column\n",
    "  if 'params' not in grid_df.columns:\n",
    "      logging.error(\"Grid DataFrame missing 'params' column.\")\n",
    "      return pd.Series(dtype=float)\n",
    "\n",
    "  # Pre-calculate the 'adj_fit' series for efficiency\n",
    "  adj_fit_series = grid_df['adj_fit']\n",
    "\n",
    "  for var_k in all_variables:\n",
    "    # 1. Find cells *with* var_k (delta_k(theta) = 1)\n",
    "    #    We check if var_k is in the 'vars' tuple within 'params'\n",
    "    try:\n",
    "        includes_k_mask = grid_df['params'].apply(lambda p: var_k in p['vars'])\n",
    "    except TypeError:\n",
    "        logging.error(f\"Error checking 'params' column. Is it in the correct format? {grid_df['params'].iloc[0]}\")\n",
    "        continue\n",
    "\n",
    "    # 2. Get average adjusted fit for cells *with* k\n",
    "    adj_fit_with_k = adj_fit_series[includes_k_mask]\n",
    "    avg_fit_with_k = adj_fit_with_k.mean()\n",
    "    if pd.isna(avg_fit_with_k):\n",
    "        avg_fit_with_k = 0.0 # Handle case where it's never used\n",
    "\n",
    "    # 3. Get average adjusted fit for cells *without* k\n",
    "    adj_fit_without_k = adj_fit_series[~includes_k_mask]\n",
    "    avg_fit_without_k = adj_fit_without_k.mean()\n",
    "    if pd.isna(avg_fit_without_k):\n",
    "        avg_fit_without_k = 0.0 # Handle case\n",
    "        \n",
    "    # 4. RBI = marginal contribution to average reliability\n",
    "    rbi_k = avg_fit_with_k - avg_fit_without_k\n",
    "    \n",
    "    rbi_scores[var_k] = rbi_k\n",
    "  \n",
    "  return pd.Series(rbi_scores)\n",
    "\n",
    "# --- Execute calculations for the single task ---\n",
    "print(\"Cell 8: Calculating composite prediction and RBI for one task...\")\n",
    "\n",
    "y_hat_grid_0, psi_weights_0 = calculate_composite_prediction(grid_for_task_0)\n",
    "\n",
    "print(f\"\\nComposite Prediction for Task 0: {y_hat_grid_0:.6f}\")\n",
    "print(\"Reliability weights (psi_weights) for top 5 grid cells:\")\n",
    "print(psi_weights_0.nlargest(5))\n",
    "\n",
    "\n",
    "rbi_scores_0 = calculate_rbi_for_task(grid_for_task_0, FEATURE_COLS)\n",
    "\n",
    "print(\"\\nRBI Scores for Task 0 (Sorted):\")\n",
    "print(rbi_scores_0.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 9: Batch Processing of All Test Prediction Tasks (Parallel)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# --- Setup check (same as before) ---\n",
    "if 'X_test' not in globals():\n",
    "    logging.error(\"Test data not found. Please run all preceding cells.\")\n",
    "    raise NameError(\"X_test is not defined. Run preceding cells.\")\n",
    "\n",
    "if 'y_test' not in globals():\n",
    "    logging.error(\"y_test not found. Please run data prep cell.\")\n",
    "    raise NameError(\"y_test is not defined. Run data prep cell.\")\n",
    "\n",
    "if 'process_grid_for_one_task' not in globals():\n",
    "    logging.error(\"process_grid_for_one_task not found. Please run Cell 7 first.\")\n",
    "    raise NameError(\"process_grid_for_one_task is not defined.\")\n",
    "\n",
    "if 'calculate_composite_prediction' not in globals():\n",
    "    logging.error(\"calculate_composite_prediction not found. Please run Cell 8 first.\")\n",
    "    raise NameError(\"calculate_composite_prediction is not defined.\")\n",
    "\n",
    "if 'calculate_rbi_for_task' not in globals():\n",
    "    logging.error(\"calculate_rbi_for_task not found. Please run Cell 8 first.\")\n",
    "    raise NameError(\"calculate_rbi_for_task is not defined.\")\n",
    "\n",
    "if 'FEATURE_COLS' not in globals():\n",
    "    logging.warning(\"FEATURE_COLS not found. Using X_train columns as fallback.\")\n",
    "    FEATURE_COLS = list(X_train.columns)\n",
    "\n",
    "# --- Helper: run one task (used in parallel) ---\n",
    "def run_single_task(task_index, x_t_row):\n",
    "    \"\"\"\n",
    "    Run full RBP+RBI pipeline for a single test task.\n",
    "    Returns (result_row_dict, rbi_scores_series) or None on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Grid processing for this task\n",
    "        grid_df = process_grid_for_one_task(x_t_row, X_train, y_train)\n",
    "\n",
    "        # 2. Composite prediction\n",
    "        y_hat_grid, _ = calculate_composite_prediction(grid_df)\n",
    "\n",
    "        # 3. RBI scores\n",
    "        rbi_scores = calculate_rbi_for_task(grid_df, FEATURE_COLS)\n",
    "        rbi_scores.name = task_index\n",
    "\n",
    "        # 4. Package results\n",
    "        result_row = {\n",
    "            \"task_index\": task_index,\n",
    "            \"y_actual\": y_test.loc[task_index],\n",
    "            \"y_pred_grid\": y_hat_grid,\n",
    "        }\n",
    "\n",
    "        return result_row, rbi_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process task {task_index}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Parallel batch processing ---\n",
    "print(f\"Starting batch processing for {len(X_test)} test tasks (parallel)...\")\n",
    "\n",
    "# Materialize the iterator so it can be reused\n",
    "task_items = list(X_test.iterrows())\n",
    "\n",
    "# n_jobs: number of worker processes.\n",
    "# -1 = all cores. You can set 4 or 8 manually if you like.\n",
    "parallel_outputs = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(run_single_task)(idx, x_t_row)\n",
    "    for idx, x_t_row in task_items\n",
    ")\n",
    "\n",
    "# Filter out any None results (errors)\n",
    "valid_outputs = [out for out in parallel_outputs if out is not None]\n",
    "\n",
    "if len(valid_outputs) == 0:\n",
    "    raise RuntimeError(\"All tasks failed â€“ check logs for errors.\")\n",
    "\n",
    "# Unzip into separate lists\n",
    "result_rows, rbi_series_list = zip(*valid_outputs)\n",
    "\n",
    "# Build DataFrames\n",
    "results_df = pd.DataFrame(result_rows).set_index(\"task_index\")\n",
    "rbi_df = pd.DataFrame(rbi_series_list)\n",
    "\n",
    "print(\"\\nBatch processing complete.\")\n",
    "print(\"\\n--- Prediction Results (Head) ---\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(\"\\n--- RBI Scores DataFrame (Head) ---\")\n",
    "print(rbi_df.head())\n",
    "results_df.to_csv(\"rbp_rbi_predictions.csv\")\n",
    "rbi_df.to_csv(\"rbp_rbi_scores.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "preds = pd.read_csv(\"rbp_rbi_predictions.csv\")\n",
    "print(preds.head(10))\n",
    "print(preds.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae242928",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv(\"rbp_rbi_scores.csv\")\n",
    "print(scores.head(10))\n",
    "print(scores.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d712695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds.columns)\n",
    "print(scores.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7611c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import statsmodels.api as sm # <-- This import is needed\n",
    "\n",
    "# --- Setup: Ensure all previous cells are run ---\n",
    "# Needs: X_train, y_train from Cell 3\n",
    "\n",
    "if 'X_train' not in globals():\n",
    "    logging.error(\"Training data not found. Please run Cell 3.\")\n",
    "    raise NameError(\"X_train is not defined.\")\n",
    "\n",
    "# --- Run a standard OLS regression on the training data ---\n",
    "# This is to get the t-statistics for comparison.\n",
    "\n",
    "def get_ols_t_stats(X_train, y_train):\n",
    "  \"\"\"\n",
    "  Runs a single OLS regression and returns the t-statistics\n",
    "  for all variables.\n",
    "  \"\"\"\n",
    "  # 1. Add a constant (intercept) to the model\n",
    "  X_with_const = sm.add_constant(X_train)\n",
    "  \n",
    "  # Align y_train with X_with_const just in case\n",
    "  y_train_aligned, X_with_const_aligned = y_train.align(X_with_const, join='inner', axis=0)\n",
    "  \n",
    "  # 2. Fit the OLS model\n",
    "  model = sm.OLS(y_train_aligned, X_with_const_aligned).fit()\n",
    "  \n",
    "  # 3. Extract t-statistics (and drop the 'const' row)\n",
    "  t_stats = model.tvalues.drop('const', errors='ignore')\n",
    "  \n",
    "  # 4. Return the absolute t-stats for ranking\n",
    "  return t_stats.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Cell 10: Calculating OLS t-statistics...\")\n",
    "ols_t_stats = get_ols_t_stats(X_train, y_train)\n",
    "\n",
    "print(\"\\n--- OLS t-statistics (Absolute) ---\")\n",
    "print(ols_t_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def to_scalar(x):\n",
    "    # Turn whatever is in the cell into a single float\n",
    "    if isinstance(x, pd.Series):\n",
    "        # take the first element if it's a Series\n",
    "        return float(x.iloc[0])\n",
    "    if isinstance(x, (np.ndarray, list, tuple)):\n",
    "        return float(np.array(x).ravel()[0])\n",
    "    return float(x)\n",
    "\n",
    "results_df_clean = results_df.copy()\n",
    "\n",
    "results_df_clean[\"y_actual\"] = results_df_clean[\"y_actual\"].map(to_scalar)\n",
    "results_df_clean[\"y_pred_grid\"] = results_df_clean[\"y_pred_grid\"].map(to_scalar)\n",
    "\n",
    "# sanity check\n",
    "print(results_df_clean.dtypes)\n",
    "print(results_df_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceebdc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sort in a sensible order\n",
    "results_df_sorted = results_df_clean.sort_index()\n",
    "\n",
    "y_true = results_df_sorted[\"y_actual\"].to_numpy()\n",
    "y_pred = results_df_sorted[\"y_pred_grid\"].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.4)\n",
    "\n",
    "# 45-degree line\n",
    "min_val = min(y_true.min(), y_pred.min())\n",
    "max_val = max(y_true.max(), y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Actual (y_actual)\")\n",
    "plt.ylabel(\"Predicted (y_pred_grid)\")\n",
    "plt.title(\"Actual vs Predicted (RBP Composite)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "results_df_sorted[\"y_actual\"].plot(label=\"Actual\", linewidth=1)\n",
    "results_df_sorted[\"y_pred_grid\"].plot(label=\"Predicted\", linewidth=1)\n",
    "\n",
    "plt.xlabel(\n",
    "    \"Task index\"\n",
    "    if not np.issubdtype(results_df_sorted.index.dtype, np.datetime64)\n",
    "    else \"Date\"\n",
    ")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.title(\"Actual vs Predicted over Tasks\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ebf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Analysis & Visualization\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_informativeness(X, x_mean, inv_cov):\n",
    "    \"\"\"\n",
    "    Calculates info(x, xÌ„) for all rows in X as the squared Mahalanobis\n",
    "    distance between each observation and the training mean xÌ„.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        DataFrame of predictors (same columns as X_train).\n",
    "    x_mean : np.ndarray\n",
    "        Mean vector of training data (shape (K,)).\n",
    "    inv_cov : np.ndarray\n",
    "        Inverse covariance matrix of training data (shape (K, K)).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Informativeness scores indexed like X (one per row).\n",
    "    \"\"\"\n",
    "    X_values = X.values\n",
    "    x_mean_vec = np.asarray(x_mean)\n",
    "    \n",
    "    info_scores = []\n",
    "    for row in X_values:\n",
    "        d2 = calculate_mahalanobis_distance(row, x_mean_vec, inv_cov)\n",
    "        info_scores.append(d2)\n",
    "    \n",
    "    return pd.Series(info_scores, index=X.index, name=\"informativeness\")\n",
    "# --- 1. Aggregate Importance Comparison (tau-like statistic) ---\n",
    "\n",
    "# 1) Informativeness scores for each test task\n",
    "info_scores_test = calculate_informativeness(\n",
    "    X_test,\n",
    "    full_train_stats['x_mean'],\n",
    "    full_train_stats['inv_cov']\n",
    ")\n",
    "\n",
    "# 2) Convert to weights (normalized informativeness)\n",
    "info_weights = info_scores_test / info_scores_test.sum()\n",
    "\n",
    "# 3) Info-weighted average RBI per variable\n",
    "#    (aligns tasks in rbi_df rows with info_weights index)\n",
    "average_rbi = rbi_df.mul(info_weights, axis=0).sum()\n",
    "\n",
    "# 4) Comparison table: OLS t-stats vs info-weighted RBI\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"t_statistic\": ols_t_stats,\n",
    "    \"info_weighted_rbi\": average_rbi\n",
    "})\n",
    "\n",
    "print(\"--- Aggregate Importance Comparison ---\")\n",
    "print(comparison_df.sort_values(by=\"info_weighted_rbi\", ascending=False))\n",
    "comparison_df.to_csv(\"importance_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e56bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Make sure rbi_df and results_df_clean are aligned on the same index\n",
    "rbi_aligned = rbi_df.reindex(results_df_clean.index)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "for col in rbi_aligned.columns:\n",
    "    if col == \"past_return_252d\":\n",
    "        # Highlight this one\n",
    "        plt.plot(\n",
    "            rbi_aligned.index,\n",
    "            rbi_aligned[col],\n",
    "            label=col,\n",
    "            linewidth=2.5,\n",
    "        )\n",
    "    else:\n",
    "        plt.plot(\n",
    "            rbi_aligned.index,\n",
    "            rbi_aligned[col],\n",
    "            label=col,\n",
    "            linewidth=1,\n",
    "            alpha=0.4,\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "\n",
    "plt.xlabel(\"Prediction Task Index\")\n",
    "plt.ylabel(\"RBI\")\n",
    "plt.title(\"Prediction-Specific RBI Paths Across Variables\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# New Cell: AAPL 10-year horse race (RBP vs OLS top feature)\n",
    "# ==========================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1. Fetch 10 years of AAPL data via FMP API\n",
    "aapl_lookback_days = 365 * 10  # ~10 years\n",
    "\n",
    "if not fmp_api_key:\n",
    "    raise ValueError(\"FMP_API_KEY not found. Please set it before running this cell.\")\n",
    "\n",
    "print(\"Fetching 10 years of AAPL data from FMP...\")\n",
    "aapl_data = _get_market_data([\"AAPL\"], aapl_lookback_days, fmp_api_key)\n",
    "\n",
    "print(f\"Fetched {len(aapl_data)} rows for AAPL.\")\n",
    "print(aapl_data.head())\n",
    "\n",
    "# 2. Engineer RBP features and target using the existing pipeline\n",
    "# Assumes engineer_features, FEATURE_COLS, TARGET_COL are already defined.\n",
    "aapl_proc = engineer_features(aapl_data.copy())\n",
    "\n",
    "# Ensure sorted by time and use timestamp as index\n",
    "aapl_proc = aapl_proc.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "X_aapl = aapl_proc[FEATURE_COLS]\n",
    "y_aapl = aapl_proc[TARGET_COL]\n",
    "\n",
    "# Drop any remaining NaNs (just in case)\n",
    "valid_mask = X_aapl.notna().all(axis=1) & y_aapl.notna()\n",
    "X_aapl = X_aapl.loc[valid_mask]\n",
    "y_aapl = y_aapl.loc[valid_mask]\n",
    "\n",
    "print(\"AAPL feature matrix shape:\", X_aapl.shape)\n",
    "\n",
    "# 3. Time-based train/test split (e.g., 70% train, 30% test)\n",
    "split_idx = int(0.7 * len(X_aapl))\n",
    "split_date = X_aapl.index[split_idx]\n",
    "\n",
    "train_mask = X_aapl.index < split_date\n",
    "test_mask = ~train_mask\n",
    "\n",
    "X_train_aapl = X_aapl.loc[train_mask]\n",
    "y_train_aapl = y_aapl.loc[train_mask]\n",
    "\n",
    "X_test_aapl = X_aapl.loc[test_mask]\n",
    "y_test_aapl = y_aapl.loc[test_mask]\n",
    "\n",
    "print(f\"Train size: {len(X_train_aapl)}, Test size: {len(X_test_aapl)}\")\n",
    "\n",
    "# 4. Choose features:\n",
    "#    - RBP top feature (info-weighted RBI): past_return_252d\n",
    "#    - OLS top feature (|t-statistic|): past_vol_21d\n",
    "rbp_feature = \"past_return_252d\"\n",
    "ols_feature = \"past_vol_21d\"\n",
    "\n",
    "assert rbp_feature in X_aapl.columns, f\"{rbp_feature} not in FEATURE_COLS\"\n",
    "assert ols_feature in X_aapl.columns, f\"{ols_feature} not in FEATURE_COLS\"\n",
    "\n",
    "Xr_train = X_train_aapl[[rbp_feature]]\n",
    "Xr_test = X_test_aapl[[rbp_feature]]\n",
    "\n",
    "Xt_train = X_train_aapl[[ols_feature]]\n",
    "Xt_test = X_test_aapl[[ols_feature]]\n",
    "\n",
    "# 5. Helper: fit univariate OLS and predict\n",
    "def fit_univariate_ols(X_train, y_train, X_test):\n",
    "    Xtr = sm.add_constant(X_train)\n",
    "    Xte = sm.add_constant(X_test)\n",
    "    model = sm.OLS(y_train, Xtr).fit()\n",
    "    y_hat = model.predict(Xte)\n",
    "    return model, y_hat\n",
    "\n",
    "model_rbp, y_hat_rbp = fit_univariate_ols(Xr_train, y_train_aapl, Xr_test)\n",
    "model_ols, y_hat_ols = fit_univariate_ols(Xt_train, y_train_aapl, Xt_test)\n",
    "\n",
    "# 6. Compute out-of-sample performance metrics\n",
    "def rmse(y_true, y_hat):\n",
    "    return float(np.sqrt(((y_true - y_hat) ** 2).mean()))\n",
    "\n",
    "def mae(y_true, y_hat):\n",
    "    return float((y_true - y_hat).abs().mean())\n",
    "\n",
    "rmse_rbp = rmse(y_test_aapl, y_hat_rbp)\n",
    "mae_rbp = mae(y_test_aapl, y_hat_rbp)\n",
    "\n",
    "rmse_ols = rmse(y_test_aapl, y_hat_ols)\n",
    "mae_ols = mae(y_test_aapl, y_hat_ols)\n",
    "\n",
    "print(\"\\n=== AAPL 10-Year Horse Race (Univariate Forecasts) ===\")\n",
    "print(f\"RBP top feature ({rbp_feature}):      RMSE = {rmse_rbp:.5f}, MAE = {mae_rbp:.5f}\")\n",
    "print(f\"OLS top feature ({ols_feature}):      RMSE = {rmse_ols:.5f}, MAE = {mae_ols:.5f}\")\n",
    "\n",
    "# 7. Plot actual vs both predictions on the test set\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Align predictions with test index\n",
    "y_hat_rbp.index = X_test_aapl.index\n",
    "y_hat_ols.index = X_test_aapl.index\n",
    "\n",
    "y_test_aapl.plot(label=\"Actual 21d Return\", linewidth=1)\n",
    "y_hat_rbp.plot(label=f\"RBP feature: {rbp_feature}\", linewidth=1)\n",
    "y_hat_ols.plot(label=f\"OLS feature: {ols_feature}\", linewidth=1, linestyle=\"--\")\n",
    "\n",
    "plt.title(\"AAPL: Out-of-Sample 21-Day Return Forecasts\\nRBP Top Feature vs OLS Top Feature\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Forward 21-Day Return\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc92b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Updated Cell: Visual magnifier + standardized time series\n",
    "# ==========================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Use the existing test series and predictions\n",
    "# Assumes: y_test_aapl, y_hat_rbp, y_hat_ols, X_test_aapl, rbp_feature, ols_feature\n",
    "\n",
    "# 1. Compute scale factors to match std dev of actual returns\n",
    "std_actual = y_test_aapl.std()\n",
    "std_rbp    = y_hat_rbp.std()\n",
    "std_ols    = y_hat_ols.std()\n",
    "\n",
    "if std_rbp == 0 or std_ols == 0:\n",
    "    raise ValueError(\"One of the prediction series has zero variance; cannot scale meaningfully.\")\n",
    "\n",
    "scale_factor_rbp = std_actual / std_rbp\n",
    "scale_factor_ols = std_actual / std_ols\n",
    "\n",
    "print(f\"RBP visual scale factor (std_actual / std_rbp) = {scale_factor_rbp:.2f}\")\n",
    "print(f\"OLS  visual scale factor (std_actual / std_ols) = {scale_factor_ols:.2f}\")\n",
    "\n",
    "# 2. Build scaled series JUST FOR PLOTTING\n",
    "y_hat_rbp_scaled = y_hat_rbp * scale_factor_rbp\n",
    "y_hat_ols_scaled = y_hat_ols * scale_factor_ols\n",
    "\n",
    "# Align indices\n",
    "y_hat_rbp_scaled.index = X_test_aapl.index\n",
    "y_hat_ols_scaled.index = X_test_aapl.index\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Plot 1: Actual vs scaled predictions (what you already had)\n",
    "# ----------------------------------------------------------\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "y_test_aapl.plot(label=\"Actual 21d Return\", linewidth=1)\n",
    "y_hat_rbp_scaled.plot(label=f\"RBP feature (scaled): {rbp_feature}\", linewidth=1)\n",
    "y_hat_ols_scaled.plot(label=f\"OLS feature (scaled): {ols_feature}\", linewidth=1, linestyle=\"--\")\n",
    "\n",
    "plt.title(\n",
    "    \"AAPL: Out-of-Sample 21-Day Return Forecasts\\n\"\n",
    "    \"RBP vs OLS Top Feature (Predictions Rescaled for Visibility)\"\n",
    ")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Forward 21-Day Return\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Plot 2: Standardized (z-score) time series of same three lines\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# z-score each series (mean 0, std 1)\n",
    "z_actual = (y_test_aapl - y_test_aapl.mean()) / y_test_aapl.std()\n",
    "z_rbp    = (y_hat_rbp - y_hat_rbp.mean()) / y_hat_rbp.std()\n",
    "z_ols    = (y_hat_ols - y_hat_ols.mean()) / y_hat_ols.std()\n",
    "\n",
    "# Align indices\n",
    "z_rbp.index = X_test_aapl.index\n",
    "z_ols.index = X_test_aapl.index\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "z_actual.plot(label=\"Actual 21d Return (z-score)\", linewidth=1)\n",
    "z_rbp.plot(label=f\"RBP feature: {rbp_feature} (z-score)\", linewidth=1)\n",
    "z_ols.plot(label=f\"OLS feature: {ols_feature} (z-score)\", linewidth=1, linestyle=\"--\")\n",
    "\n",
    "plt.title(\n",
    "    \"AAPL: Standardized Time Series\\n\"\n",
    "    \"Actual vs RBP and OLS Top Features (z-scored)\"\n",
    ")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Standardized Value (z-score)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MQS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
