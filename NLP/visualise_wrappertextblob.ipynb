{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your env already has these, you can skip this cell.\n",
    "# pip install -U transformers datasets scikit-learn accelerate textblob matplotlib pandas numpy\n",
    "# pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a0af4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.backends.registry'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m f1_score, accuracy_score, precision_recall_fscore_support, mean_absolute_error\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# our wrapper (inference helpers + TextBlob baseline)\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magenticwrapper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     LocalAgenticNewsAI,\n\u001b[32m     15\u001b[39m     load_agent, score_batch_with_agent,\n\u001b[32m     16\u001b[39m     normalize_articles_df, to_finbert_like_columns\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MUNQS\\MQSMaster\\MQS\\Lib\\site-packages\\matplotlib\\__init__.py:161\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrcsetup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MUNQS\\MQSMaster\\MQS\\Lib\\site-packages\\matplotlib\\rcsetup.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MUNQS\\MQSMaster\\MQS\\Lib\\site-packages\\matplotlib\\backends\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# NOTE: plt.switch_backend() (called at import time) will add a \"backend\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# attribute here for backcompat.\u001b[39;00m\n\u001b[32m      5\u001b[39m _QT_FORCE_QT5_BINDING = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib.backends.registry'"
     ]
    }
   ],
   "source": [
    "import os, json, math, random, hashlib, pickle\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# our wrapper (inference helpers + TextBlob baseline)\n",
    "from agenticwrapper import (\n",
    "    LocalAgenticNewsAI,\n",
    "    load_agent, score_batch_with_agent,\n",
    "    normalize_articles_df, to_finbert_like_columns\n",
    ")\n",
    "\n",
    "# ---------- project paths ----------\n",
    "NLP_DIR = Path(\"NLP\")\n",
    "ART_DIR = NLP_DIR / \"articles\"                  # fetch_articles.py writes here\n",
    "OUT_DIR = NLP_DIR / \"sentiment_scores_agentic\"  # new outputs here\n",
    "MODEL_DIR = NLP_DIR / \"models\"                  # where checkpoints live\n",
    "CACHE_DIR = NLP_DIR / \"cache\"                   # notebook-level caches\n",
    "DATA_DIR  = NLP_DIR / \"data\"                    # labeled train/valid CSVs (0=neg,1=neu,2=pos)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- experiment knobs ----------\n",
    "TICKERS = [\"AAPL\"]              # change / extend as needed\n",
    "ENGINE  = \"agent\"               # \"textblob\" or \"agent\"\n",
    "BASE_MODEL = str(MODEL_DIR / \"finbert-finetuned-final\")  # or a HF name like \"ProsusAI/finbert\"\n",
    "TRAIN_CSV  = str(DATA_DIR / \"train.csv\")  # must have columns: text, label (0/1/2)\n",
    "VALID_CSV  = str(DATA_DIR / \"valid.csv\")\n",
    "CHECKPOINT_OUT = str(MODEL_DIR / \"trained_sentiment_agent\")    # supervised output\n",
    "EVO_ROOT       = str(MODEL_DIR / \"evolved_sentiment_agent\")    # evolutionary runs\n",
    "\n",
    "# scoring params\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# switches (set True only when you want to run training/tuning)\n",
    "RUN_TRAIN = False         # supervised fine-tune\n",
    "RUN_EVO   = False         # evolutionary hyperparam search\n",
    "RUN_BANDIT= False         # threshold tuning on validation set\n",
    "\n",
    "# label maps\n",
    "id2label = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "print(\"device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42369524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "class SimpleHFDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tok, max_len):\n",
    "        self.texts, self.labels, self.tok, self.max_len = texts, labels, tok, max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(self.texts[i], truncation=True, padding=\"max_length\",\n",
    "                       max_length=self.max_len, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"][0],\n",
    "            \"attention_mask\": enc[\"attention_mask\"][0],\n",
    "            \"labels\": torch.tensor(self.labels[i], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return {\"accuracy\": acc, \"macro_f1\": f1, \"precision\": prec, \"recall\": rec}\n",
    "\n",
    "def finetune_supervised(\n",
    "    base_model: str,\n",
    "    train_csv: str,\n",
    "    valid_csv: str,\n",
    "    output_dir: str,\n",
    "    text_col: str = \"text\",\n",
    "    label_col: str = \"label\",\n",
    "    max_length: int = 256,\n",
    "    lr: float = 2e-5,\n",
    "    weight_decay: float = 0.01,\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 16,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    seed_everything(seed)\n",
    "    tok = AutoTokenizer.from_pretrained(base_model)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model, num_labels=3, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "    df_tr = pd.read_csv(train_csv); df_va = pd.read_csv(valid_csv)\n",
    "    tr_ds = SimpleHFDS(df_tr[text_col].astype(str).tolist(), df_tr[label_col].tolist(), tok, max_length)\n",
    "    va_ds = SimpleHFDS(df_va[text_col].astype(str).tolist(), df_va[label_col].tolist(), tok, max_length)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=lr, weight_decay=weight_decay,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        seed=seed, logging_steps=50\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=mdl, args=args, train_dataset=tr_ds, eval_dataset=va_ds,\n",
    "        compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tok.save_pretrained(output_dir)\n",
    "    print(f\"[TRAIN] saved checkpoint -> {output_dir}\")\n",
    "    return output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e5d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvoConfig:\n",
    "    population: int = 6\n",
    "    generations: int = 2\n",
    "    lr_range: Tuple[float,float] = (1e-5, 5e-5)\n",
    "    wd_range: Tuple[float,float] = (0.0, 0.1)\n",
    "    maxlen_choices: Tuple[int,...] = (128, 256, 384)\n",
    "    batch_choices: Tuple[int,...] = (8, 16, 32)\n",
    "    seed: int = 13\n",
    "\n",
    "def _sample_hparams(cfg: EvoConfig):\n",
    "    lr = 10 ** np.random.uniform(np.log10(cfg.lr_range[0]), np.log10(cfg.lr_range[1]))\n",
    "    wd = np.random.uniform(cfg.wd_range[0], cfg.wd_range[1])\n",
    "    L  = random.choice(cfg.maxlen_choices)\n",
    "    bs = random.choice(cfg.batch_choices)\n",
    "    return dict(lr=lr, weight_decay=wd, max_length=L, batch_size=bs)\n",
    "\n",
    "def _valid_macro_f1(checkpoint_dir: str, valid_csv: str, text_col=\"text\", label_col=\"label\",\n",
    "                    max_length=256, batch_size=32):\n",
    "    tok, mdl, dev, id2 = load_agent(checkpoint_dir, device=DEVICE)\n",
    "    df = pd.read_csv(valid_csv)\n",
    "    texts = df[text_col].astype(str).tolist()\n",
    "    # run logits->probs\n",
    "    scores, labels = score_batch_with_agent(\n",
    "        texts, tok, mdl, dev, id2, max_length=max_length, batch_size=batch_size\n",
    "    )\n",
    "    # convert predicted label names to ids\n",
    "    name2id = {v.lower():k for k,v in id2.items()}\n",
    "    pred_ids = [name2id[l] for l in labels]\n",
    "    return f1_score(df[label_col].tolist(), pred_ids, average=\"macro\")\n",
    "\n",
    "def evolutionary_search(\n",
    "    base_model: str,\n",
    "    train_csv: str, valid_csv: str,\n",
    "    text_col: str = \"text\", label_col: str = \"label\",\n",
    "    evo: EvoConfig = EvoConfig(),\n",
    "    out_root: str = EVO_ROOT\n",
    "):\n",
    "    seed_everything(evo.seed)\n",
    "    best = {\"macro_f1\": -1.0, \"dir\": None, \"hp\": None}\n",
    "    for gen in range(evo.generations):\n",
    "        print(f\"[EVO] Generation {gen+1}/{evo.generations}\")\n",
    "        candidates = [_sample_hparams(evo) for _ in range(evo.population)]\n",
    "        for i, hp in enumerate(candidates):\n",
    "            out_dir = str(Path(out_root) / f\"gen{gen}_cand{i}\")\n",
    "            print(f\"  [cand] {hp} -> {out_dir}\")\n",
    "            ckpt = finetune_supervised(\n",
    "                base_model, train_csv, valid_csv, out_dir,\n",
    "                text_col=text_col, label_col=label_col,\n",
    "                max_length=hp[\"max_length\"], lr=hp[\"lr\"], weight_decay=hp[\"weight_decay\"],\n",
    "                epochs=3, batch_size=hp[\"batch_size\"], seed=evo.seed + gen*10 + i\n",
    "            )\n",
    "            f1 = _valid_macro_f1(ckpt, valid_csv, text_col, label_col,\n",
    "                                  max_length=hp[\"max_length\"], batch_size=hp[\"batch_size\"])\n",
    "            print(f\"    [val] macro-F1 = {f1:.4f}\")\n",
    "            if f1 > best[\"macro_f1\"]:\n",
    "                best = {\"macro_f1\": f1, \"dir\": ckpt, \"hp\": hp}\n",
    "    print(\"[EVO] best:\", best)\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softmax_np(logits: np.ndarray) -> np.ndarray:\n",
    "    z = logits - logits.max(axis=-1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / e.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def collect_valid_probs(checkpoint_dir: str, valid_csv: str, text_col=\"text\", max_length=256, batch_size=32):\n",
    "    tok, mdl, dev, id2 = load_agent(checkpoint_dir, device=DEVICE)\n",
    "    df = pd.read_csv(valid_csv)\n",
    "    all_probs = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        chunk = df.iloc[i:i+batch_size]\n",
    "        enc = tok(chunk[text_col].astype(str).tolist(), padding=True, truncation=True,\n",
    "                  max_length=max_length, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(dev) for k,v in enc.items()}\n",
    "        with torch.inference_mode():\n",
    "            logits = mdl(**enc).logits.detach().cpu().numpy()\n",
    "        probs = _softmax_np(logits)\n",
    "        all_probs.append(probs)\n",
    "    probs = np.vstack(all_probs)\n",
    "    return probs, df[\"label\"].tolist(), id2\n",
    "\n",
    "def bandit_threshold_tuner(\n",
    "    probs: np.ndarray, labels_true: List[int], id2label: Dict[int,str],\n",
    "    iters: int = 2000, eps: float = 0.1, seed: int = 7\n",
    "):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    # map prob rows -> continuous scores with the same formula used in inference\n",
    "    def probs_to_score(p):\n",
    "        by = {id2label[i].lower(): float(p[i]) for i in range(len(p))}\n",
    "        return (by.get(\"positive\",0)-by.get(\"negative\",0)) * (1.0 - by.get(\"neutral\",0))\n",
    "    scores = np.apply_along_axis(probs_to_score, 1, probs)\n",
    "\n",
    "    grid = np.linspace(-0.9, 0.9, 73)  # candidate thresholds\n",
    "    best = {\"f1\": -1.0, \"tau_neg\": 0.0, \"tau_pos\": 0.0}\n",
    "    for _ in range(iters):\n",
    "        if random.random() < eps:\n",
    "            tneg = float(np.random.choice(grid))\n",
    "            tpos = float(np.random.choice(grid))\n",
    "        else:\n",
    "            # small exploitation around best thresholds\n",
    "            tneg = best[\"tau_neg\"] + float(np.random.randn()*0.05)\n",
    "            tpos = best[\"tau_pos\"] + float(np.random.randn()*0.05)\n",
    "        pred = np.where(scores >= tpos, 2, np.where(scores <= tneg, 0, 1))\n",
    "        f1 = f1_score(labels_true, pred, average=\"macro\")\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"f1\": f1, \"tau_neg\": tneg, \"tau_pos\": tpos}\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_text(s: str) -> str:\n",
    "    return hashlib.sha256((s or \"\").encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "def load_cache(path: Path) -> Dict[str, Tuple[float, str]]:\n",
    "    if path.exists():\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache: Dict[str, Tuple[float, str]], path: Path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "def score_articles_with_cache(\n",
    "    texts: List[str],\n",
    "    model_dir: str,\n",
    "    cache_path: Path,\n",
    "    device: str = DEVICE,\n",
    "    max_length: int = MAX_LENGTH,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    # Use the inference helpers from agenticwrapper, but skip cached items\n",
    "    tok, mdl, dev, id2 = load_agent(model_dir, device=device)\n",
    "    cache = load_cache(cache_path)\n",
    "\n",
    "    scores, labels = [], []\n",
    "    miss_idx, miss_texts = [], []\n",
    "\n",
    "    # hit cache where possible\n",
    "    for i, t in enumerate(texts):\n",
    "        key = hash_text(t)\n",
    "        if key in cache:\n",
    "            s, lb = cache[key]\n",
    "            scores.append(s); labels.append(lb)\n",
    "        else:\n",
    "            scores.append(None); labels.append(None)\n",
    "            miss_idx.append(i); miss_texts.append(t)\n",
    "\n",
    "    # score misses in batches\n",
    "    if miss_texts:\n",
    "        miss_scores, miss_labels = score_batch_with_agent(\n",
    "            miss_texts, tok, mdl, dev, id2,\n",
    "            max_length=max_length, batch_size=batch_size\n",
    "        )\n",
    "        for k, (s, lb) in enumerate(zip(miss_scores, miss_labels)):\n",
    "            i = miss_idx[k]\n",
    "            scores[i] = s; labels[i] = lb\n",
    "            cache[hash_text(miss_texts[k])] = (s, lb)\n",
    "\n",
    "        save_cache(cache, cache_path)\n",
    "\n",
    "    return scores, labels\n",
    "\n",
    "def add_finbert_parity_columns(df_articles: pd.DataFrame) -> pd.DataFrame:\n",
    "    # if TextBlob columns exist, add sentiment_score so downstream plots align\n",
    "    return to_finbert_like_columns(df_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint_dir = None  # we'll fill this if any of the runs happen\n",
    "\n",
    "if RUN_TRAIN:\n",
    "    best_checkpoint_dir = finetune_supervised(\n",
    "        base_model=BASE_MODEL,\n",
    "        train_csv=TRAIN_CSV, valid_csv=VALID_CSV,\n",
    "        output_dir=CHECKPOINT_OUT,\n",
    "        max_length=MAX_LENGTH, lr=2e-5, weight_decay=0.01,\n",
    "        epochs=3, batch_size=16, seed=42\n",
    "    )\n",
    "\n",
    "if RUN_EVO:\n",
    "    evo_best = evolutionary_search(\n",
    "        base_model=BASE_MODEL, train_csv=TRAIN_CSV, valid_csv=VALID_CSV,\n",
    "        text_col=\"text\", label_col=\"label\",\n",
    "        evo=EvoConfig(population=6, generations=2),\n",
    "        out_root=EVO_ROOT\n",
    "    )\n",
    "    best_checkpoint_dir = evo_best[\"dir\"]\n",
    "\n",
    "if RUN_BANDIT:\n",
    "    assert best_checkpoint_dir, \"Bandit tuner needs a trained checkpoint (set RUN_TRAIN or RUN_EVO first).\"\n",
    "    probs, y_true, id2 = collect_valid_probs(\n",
    "        best_checkpoint_dir, VALID_CSV, text_col=\"text\",\n",
    "        max_length=MAX_LENGTH, batch_size=BATCH_SIZE\n",
    "    )\n",
    "    bandit = bandit_threshold_tuner(probs, y_true, id2, iters=2000, eps=0.1)\n",
    "    print(\"[BANDIT] best:\", bandit)\n",
    "\n",
    "# Pick a checkpoint for inference:\n",
    "AGENT_MODEL_DIR = best_checkpoint_dir or CHECKPOINT_OUT  # or manually set to any saved dir\n",
    "print(\"AGENT_MODEL_DIR =\", AGENT_MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d89a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles_for_ticker(ticker: str) -> pd.DataFrame:\n",
    "    csv_path = ART_DIR / f\"{ticker}.csv\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing articles CSV: {csv_path}\")\n",
    "    raw = pd.read_csv(csv_path)\n",
    "    df = normalize_articles_df(raw)  # ensures {date,title,content}\n",
    "    return df\n",
    "\n",
    "articles_by_ticker: Dict[str, pd.DataFrame] = {}\n",
    "for t in TICKERS:\n",
    "    df = load_articles_for_ticker(t)\n",
    "    print(f\"{t}: {len(df)} articles\")\n",
    "    articles_by_ticker[t] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4437065",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for t, df_norm in articles_by_ticker.items():\n",
    "    agent = LocalAgenticNewsAI()\n",
    "    agent.memory = df_norm.copy()\n",
    "\n",
    "    if ENGINE == \"textblob\":\n",
    "        agent.analyze_sentiment()\n",
    "        per_article = add_finbert_parity_columns(agent.memory.copy())  # adds sentiment_score=polarity\n",
    "        # TextBlob daily:\n",
    "        daily = (per_article.groupby(\"date\", as_index=False)\n",
    "                 .agg(mean_score=(\"sentiment_score\",\"mean\"),\n",
    "                      n_articles=(\"sentiment_score\",\"size\"),\n",
    "                      pos_share=(\"sentiment_score\", lambda s: float((s>0).mean()))))\n",
    "    else:\n",
    "        # agent inference with on-disk cache to avoid rescoring repeats\n",
    "        cache_path = CACHE_DIR / f\"{t}__agent_cache.pkl\"\n",
    "        texts = agent.memory[\"content\"].astype(str).tolist()\n",
    "        scores, labels = score_articles_with_cache(\n",
    "            texts, model_dir=str(AGENT_MODEL_DIR), cache_path=cache_path,\n",
    "            device=DEVICE, max_length=MAX_LENGTH, batch_size=BATCH_SIZE\n",
    "        )\n",
    "        per_article = agent.memory.copy()\n",
    "        per_article[\"sentiment_score\"] = scores\n",
    "        per_article[\"sentiment_label\"] = labels\n",
    "        daily = (per_article.groupby(\"date\", as_index=False)\n",
    "                 .agg(mean_score=(\"sentiment_score\",\"mean\"),\n",
    "                      n_articles=(\"sentiment_score\",\"size\"),\n",
    "                      pos_share=(\"sentiment_label\", lambda s: float((s==\"positive\").mean()))))\n",
    "\n",
    "    # save outputs\n",
    "    art_out   = OUT_DIR / f\"{t}__agent_articles.csv\"\n",
    "    daily_out = OUT_DIR / f\"{t}__agent_daily.csv\"\n",
    "    per_article.to_csv(art_out, index=False)\n",
    "    daily.to_csv(daily_out, index=False)\n",
    "    print(\"Saved:\", art_out, \"|\", daily_out)\n",
    "\n",
    "    results[t] = {\"articles\": per_article, \"daily\": daily}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b567c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_load_finbert_daily(ticker: str) -> Optional[pd.DataFrame]:\n",
    "    # Try to auto-detect FinBERT daily file (adjust to your exact path if needed)\n",
    "    candidates = [\n",
    "        NLP_DIR / \"sentiment_scores\" / f\"{ticker}__daily.csv\",\n",
    "        NLP_DIR / \"sentiment_scores\" / f\"{ticker}_daily.csv\",\n",
    "        NLP_DIR / \"sentiment_scores\" / f\"{ticker}__finbert_daily.csv\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            df = pd.read_csv(p)\n",
    "            # unify column name\n",
    "            if \"mean_score\" not in df.columns and \"score\" in df.columns:\n",
    "                df = df.rename(columns={\"score\":\"mean_score\"})\n",
    "            return df\n",
    "    return None\n",
    "\n",
    "for t, d in results.items():\n",
    "    daily = d[\"daily\"].copy()\n",
    "    daily[\"date\"] = pd.to_datetime(daily[\"date\"])\n",
    "    plt.figure()\n",
    "    plt.plot(daily[\"date\"], daily[\"mean_score\"])\n",
    "    plt.title(f\"{t} — Agentic daily mean_score\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"mean_score\"); plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    fb = maybe_load_finbert_daily(t)\n",
    "    if fb is not None:\n",
    "        fb[\"date\"] = pd.to_datetime(fb[\"date\"])\n",
    "        merged = pd.merge(daily[[\"date\",\"mean_score\"]], fb[[\"date\",\"mean_score\"]],\n",
    "                          on=\"date\", how=\"inner\", suffixes=(\"_agent\",\"_finbert\"))\n",
    "        if not merged.empty:\n",
    "            corr = merged[\"mean_score_agent\"].corr(merged[\"mean_score_finbert\"])\n",
    "            mae  = mean_absolute_error(merged[\"mean_score_finbert\"], merged[\"mean_score_agent\"])\n",
    "            print(f\"[{t}] correlation (agent vs FinBERT): {corr:.3f} | MAE: {mae:.4f}\")\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(merged[\"date\"], merged[\"mean_score_agent\"], label=\"Agent\")\n",
    "            plt.plot(merged[\"date\"], merged[\"mean_score_finbert\"], label=\"FinBERT\", linestyle=\"--\")\n",
    "            plt.title(f\"{t} — Daily Sentiment (Agent vs FinBERT)\")\n",
    "            plt.xlabel(\"Date\"); plt.ylabel(\"mean_score\"); plt.legend(); plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"[{t}] No overlapping dates with FinBERT daily file.\")\n",
    "    else:\n",
    "        print(f\"[{t}] FinBERT daily file not found; skipped comparison.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "for t, d in results.items():\n",
    "    daily = d[\"daily\"]\n",
    "    summary_rows.append({\n",
    "        \"ticker\": t,\n",
    "        \"n_days\": len(daily),\n",
    "        \"mean_of_meanscore\": float(daily[\"mean_score\"].mean()),\n",
    "        \"stdev_of_meanscore\": float(daily[\"mean_score\"].std(ddof=0)),\n",
    "        \"avg_articles_per_day\": float(daily[\"n_articles\"].mean()),\n",
    "        \"avg_pos_share\": float(daily[\"pos_share\"].mean())\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_path = OUT_DIR / \"run_summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "summary_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MQS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
