{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sentiment.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ─── resolve script_dir & add project root ────────────────────────────────────\n",
    "try:\n",
    "    script_dir = Path(__file__).parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "\n",
    "proj_root = script_dir.parent\n",
    "if str(proj_root) not in sys.path:\n",
    "    sys.path.insert(0, str(proj_root))\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "API_KEY    = os.getenv(\"FMP_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Missing FMP_API_KEY environment variable. \"\n",
    "                       \"Please set it to your FinancialModelingPrep API key.\")\n",
    "\n",
    "MODEL_PATH = script_dir / \"finbert-finetuned-final\"\n",
    "ART_DIR    = script_dir / \"articles\"\n",
    "\n",
    "# ─── HELPERS ──────────────────────────────────────────────────────────────────\n",
    "def load_articles(ticker: str) -> pd.DataFrame:\n",
    "    path = ART_DIR / f\"{ticker}.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No article CSV found at {path}\")\n",
    "    df = pd.read_csv(path, parse_dates=[\"publishedDate\"])\n",
    "    df[\"date\"] = df[\"publishedDate\"].dt.date\n",
    "    return df\n",
    "\n",
    "def analyze_sentiment(df: pd.DataFrame, tokenizer, model) -> pd.DataFrame:\n",
    "    recs = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.get(\"content\") or row.get(\"title\", \"\")\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            continue\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "        recs.append({\n",
    "            \"date\": row[\"date\"],\n",
    "            \"sentiment\": probs[0].item() - probs[2].item()\n",
    "        })\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "def fetch_prices(ticker: str) -> pd.DataFrame:\n",
    "    url = (\n",
    "        f\"https://financialmodelingprep.com/api/v3/\"\n",
    "        f\"historical-price-full/{ticker}?apikey={API_KEY}\"\n",
    "    )\n",
    "    resp = requests.get(url, timeout=10)\n",
    "    if resp.status_code == 401:\n",
    "        raise RuntimeError(\"Unauthorized fetching prices: check your FMP_API_KEY.\")\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json().get(\"historical\", [])\n",
    "    if not data:\n",
    "        raise ValueError(f\"No price history returned for {ticker}.\")\n",
    "    prices = pd.DataFrame(data)\n",
    "    prices[\"date\"] = pd.to_datetime(prices[\"date\"])\n",
    "    return prices[[\"date\", \"close\"]]\n",
    "\n",
    "# ─── MAIN ─────────────────────────────────────────────────────────────────────\n",
    "def main(ticker: str):\n",
    "    # 1. load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH))\n",
    "    model     = AutoModelForSequenceClassification.from_pretrained(str(MODEL_PATH))\n",
    "    model.eval()\n",
    "\n",
    "    # 2. score articles\n",
    "    art_df  = load_articles(ticker)\n",
    "    sent_df = analyze_sentiment(art_df, tokenizer, model)\n",
    "    if sent_df.empty:\n",
    "        print(f\"No valid articles found for {ticker}.\")\n",
    "        return\n",
    "\n",
    "    # 3. daily sentiment\n",
    "    daily = (\n",
    "        sent_df\n",
    "        .groupby(\"date\")[\"sentiment\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .assign(date=lambda df: pd.to_datetime(df[\"date\"]))\n",
    "    )\n",
    "\n",
    "    # 4. fetch & merge prices\n",
    "    price_df = fetch_prices(ticker)\n",
    "    merged   = price_df.merge(daily, on=\"date\", how=\"left\")\n",
    "    merged[\"sentiment\"].fillna(0, inplace=True)\n",
    "    merged[\"sentiment_ma\"] = merged[\"sentiment\"].rolling(7, min_periods=1).mean()\n",
    "\n",
    "    # 5. plot\n",
    "    fig, ax1 = plt.subplots(figsize=(14,7))\n",
    "    ax1.plot(merged[\"date\"], merged[\"close\"], label=\"Close Price\")\n",
    "    ax1.set_xlabel(\"Date\"); ax1.set_ylabel(\"Price\")\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(\n",
    "        merged[\"date\"],\n",
    "        merged[\"sentiment_ma\"],\n",
    "        color=\"tab:red\",\n",
    "        label=\"7-Day Sentiment MA\"\n",
    "    )\n",
    "    ax2.set_ylabel(\"Sentiment Score\")\n",
    "    ax2.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "\n",
    "    # combined legend & title\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1+h2, l1+l2, loc=\"upper left\")\n",
    "    plt.title(f\"{ticker}: Price vs. Sentiment\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ─── ENTRY POINT ──────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    ticker = input(\"Enter ticker (e.g. AAPL): \").strip().upper()\n",
    "    try:\n",
    "        main(ticker)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d6e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sentiment.py\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ─── resolve script_dir & add project root ────────────────────────────────────\n",
    "try:\n",
    "    script_dir = Path(__file__).parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "\n",
    "proj_root = script_dir.parent\n",
    "if str(proj_root) not in sys.path:\n",
    "    sys.path.insert(0, str(proj_root))\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "API_KEY = os.getenv(\"FMP_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Missing FMP_API_KEY environment variable. \"\n",
    "                      \"Please set it to your FinancialModelingPrep API key.\")\n",
    "\n",
    "MODEL_PATH = script_dir / \"finbert-finetuned-final\"\n",
    "ART_DIR = script_dir / \"articles\"\n",
    "\n",
    "# ─── HELPERS ──────────────────────────────────────────────────────────────────\n",
    "def load_articles(ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"Load news articles for given ticker\"\"\"\n",
    "    path = ART_DIR / f\"{ticker}.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No article CSV found at {path}\")\n",
    "    df = pd.read_csv(path, parse_dates=[\"publishedDate\"])\n",
    "    df[\"date\"] = df[\"publishedDate\"].dt.date\n",
    "    return df\n",
    "\n",
    "def analyze_sentiment(df: pd.DataFrame, tokenizer, model) -> pd.DataFrame:\n",
    "    \"\"\"Analyze sentiment of news articles\"\"\"\n",
    "    recs = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.get(\"content\") or row.get(\"title\", \"\")\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            continue\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "        recs.append({\n",
    "            \"date\": row[\"date\"],\n",
    "            \"sentiment\": probs[0].item() - probs[2].item()  # positive - negative\n",
    "        })\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "def fetch_prices(ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch historical stock prices\"\"\"\n",
    "    url = (\n",
    "        f\"https://financialmodelingprep.com/api/v3/\"\n",
    "        f\"historical-price-full/{ticker}?apikey={API_KEY}\"\n",
    "    )\n",
    "    resp = requests.get(url, timeout=10)\n",
    "    if resp.status_code == 401:\n",
    "        raise RuntimeError(\"Unauthorized fetching prices: check your FMP_API_KEY.\")\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json().get(\"historical\", [])\n",
    "    if not data:\n",
    "        raise ValueError(f\"No price history returned for {ticker}.\")\n",
    "    prices = pd.DataFrame(data)\n",
    "    prices[\"date\"] = pd.to_datetime(prices[\"date\"])\n",
    "    return prices[[\"date\", \"close\"]]\n",
    "\n",
    "def format_plot(ax1, ax2, ticker, sentiment_start, sentiment_end):\n",
    "    \"\"\"Format the plot with consistent styling\"\"\"\n",
    "    # Set title and labels\n",
    "    ax1.set_title(f\"{ticker} Stock Price vs. News Sentiment\", pad=20)\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Grid and colors\n",
    "    ax1.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax1.set_ylabel('Stock Price', color='b')\n",
    "    ax1.tick_params(axis='y', colors='b')\n",
    "    \n",
    "    ax2.set_ylabel('Sentiment Score (7-day MA)', color='r')\n",
    "    ax2.tick_params(axis='y', colors='r')\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax2.axhline(0, color='gray', linestyle='--', linewidth=1, label='Neutral')\n",
    "    \n",
    "    # Add sentiment period markers if within plot range\n",
    "    xlim = ax1.get_xlim()\n",
    "    plot_start = mdates.num2date(xlim[0]).date()\n",
    "    plot_end = mdates.num2date(xlim[1]).date()\n",
    "    \n",
    "    if isinstance(sentiment_start, datetime):\n",
    "        sentiment_start = sentiment_start.date()\n",
    "    if isinstance(sentiment_end, datetime):\n",
    "        sentiment_end = sentiment_end.date()\n",
    "    \n",
    "    if sentiment_start >= plot_start and sentiment_start <= plot_end:\n",
    "        ax2.axvline(pd.to_datetime(sentiment_start), \n",
    "                   color='g', linestyle=':', alpha=0.7, label='Sentiment Start')\n",
    "    if sentiment_end >= plot_start and sentiment_end <= plot_end:\n",
    "        ax2.axvline(pd.to_datetime(sentiment_end), \n",
    "                   color='r', linestyle=':', alpha=0.7, label='Sentiment End')\n",
    "\n",
    "# ─── MAIN ─────────────────────────────────────────────────────────────────────\n",
    "def main(ticker: str, start_date=None, end_date=None):\n",
    "    \"\"\"Main analysis and plotting function\"\"\"\n",
    "    # 1. Load model and tokenizer\n",
    "    print(\"Loading sentiment model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH))\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(str(MODEL_PATH))\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Score articles\n",
    "    print(\"Loading and analyzing articles...\")\n",
    "    try:\n",
    "        art_df = load_articles(ticker)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    sent_df = analyze_sentiment(art_df, tokenizer, model)\n",
    "    if sent_df.empty:\n",
    "        print(f\"No valid articles found for {ticker}.\")\n",
    "        return\n",
    "\n",
    "    sentiment_start = sent_df['date'].min()\n",
    "    sentiment_end = sent_df['date'].max()\n",
    "\n",
    "    # 3. Calculate daily sentiment\n",
    "    print(\"Calculating daily sentiment...\")\n",
    "    daily = (\n",
    "        sent_df\n",
    "        .groupby(\"date\")[\"sentiment\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .assign(date=lambda df: pd.to_datetime(df[\"date\"]))\n",
    "    )\n",
    "\n",
    "    # 4. Fetch and merge prices\n",
    "    print(\"Fetching price data...\")\n",
    "    try:\n",
    "        price_df = fetch_prices(ticker)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching prices: {e}\")\n",
    "        return\n",
    "\n",
    "    merged = price_df.merge(daily, on=\"date\", how=\"left\")\n",
    "    merged[\"sentiment\"].fillna(0, inplace=True)  # Neutral for days without articles\n",
    "    merged[\"sentiment_ma\"] = merged[\"sentiment\"].rolling(7, min_periods=1).mean()\n",
    "\n",
    "    # Filter by date range if specified\n",
    "    if start_date:\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        merged = merged[merged['date'] >= start_date]\n",
    "    if end_date:\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "        merged = merged[merged['date'] <= end_date]\n",
    "\n",
    "    if merged.empty:\n",
    "        print(\"No data available for the specified date range.\")\n",
    "        return\n",
    "\n",
    "    # 5. Create plot\n",
    "    print(\"Generating plot...\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    # Plot stock price\n",
    "    ax1.plot(merged['date'], merged['close'], \n",
    "             'b-', linewidth=2, label='Stock Price')\n",
    "    \n",
    "    # Create twin axis for sentiment\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(merged['date'], merged['sentiment_ma'], \n",
    "             'r-', linewidth=2, label='7-day Sentiment Avg')\n",
    "    \n",
    "    # Format the plot\n",
    "    format_plot(ax1, ax2, ticker, sentiment_start, sentiment_end)\n",
    "    \n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ─── ENTRY POINT ──────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    ticker = input(\"Enter ticker (e.g. AAPL): \").strip().upper()\n",
    "    try:\n",
    "        # Example: main(ticker, start_date='2025-04-15', end_date='2025-07-15')\n",
    "        main(ticker)  # Use without date range for full history\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5525f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sentiment_combined.py\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ─── resolve script_dir & add project root ────────────────────────────────────\n",
    "try:\n",
    "    script_dir = Path(__file__).parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "\n",
    "proj_root = script_dir.parent\n",
    "if str(proj_root) not in sys.path:\n",
    "    sys.path.insert(0, str(proj_root))\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "API_KEY = os.getenv(\"FMP_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Missing FMP_API_KEY environment variable. \"\n",
    "                      \"Please set it to your FinancialModelingPrep API key.\")\n",
    "\n",
    "MODEL_PATH = script_dir / \"finbert-finetuned-final\"\n",
    "ART_DIR = script_dir / \"articles\"\n",
    "\n",
    "# ─── HELPERS ──────────────────────────────────────────────────────────────────\n",
    "def load_articles(ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"Load news articles for given ticker\"\"\"\n",
    "    path = ART_DIR / f\"{ticker}.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No article CSV found at {path}\")\n",
    "    df = pd.read_csv(path, parse_dates=[\"publishedDate\"])\n",
    "    df[\"date\"] = df[\"publishedDate\"].dt.date\n",
    "    return df\n",
    "\n",
    "def analyze_sentiment(df: pd.DataFrame, tokenizer, model) -> pd.DataFrame:\n",
    "    \"\"\"Analyze sentiment of news articles\"\"\"\n",
    "    recs = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.get(\"content\") or row.get(\"title\", \"\")\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            continue\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "        recs.append({\n",
    "            \"date\": row[\"date\"],\n",
    "            \"sentiment\": probs[0].item() - probs[2].item(),  # positive - negative\n",
    "            \"title\": row.get(\"title\", \"\"),\n",
    "            \"source\": row.get(\"site\", \"Unknown\")\n",
    "        })\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "def fetch_prices(ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch historical stock prices\"\"\"\n",
    "    url = (\n",
    "        f\"https://financialmodelingprep.com/api/v3/\"\n",
    "        f\"historical-price-full/{ticker}?apikey={API_KEY}\"\n",
    "    )\n",
    "    resp = requests.get(url, timeout=10)\n",
    "    if resp.status_code == 401:\n",
    "        raise RuntimeError(\"Unauthorized fetching prices: check your FMP_API_KEY.\")\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json().get(\"historical\", [])\n",
    "    if not data:\n",
    "        raise ValueError(f\"No price history returned for {ticker}.\")\n",
    "    prices = pd.DataFrame(data)\n",
    "    prices[\"date\"] = pd.to_datetime(prices[\"date\"])\n",
    "    return prices[[\"date\", \"close\"]]\n",
    "\n",
    "# ─── MAIN ─────────────────────────────────────────────────────────────────────\n",
    "def main(ticker: str, start_date=None, end_date=None):\n",
    "    \"\"\"Main analysis and plotting function\"\"\"\n",
    "    # 1. Load model and tokenizer\n",
    "    print(\"Loading sentiment model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH))\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(str(MODEL_PATH))\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Score articles\n",
    "    print(\"Loading and analyzing articles...\")\n",
    "    try:\n",
    "        art_df = load_articles(ticker)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    sent_df = analyze_sentiment(art_df, tokenizer, model)\n",
    "    if sent_df.empty:\n",
    "        print(f\"No valid articles found for {ticker}.\")\n",
    "        return\n",
    "\n",
    "    sentiment_start = sent_df['date'].min()\n",
    "    sentiment_end = sent_df['date'].max()\n",
    "\n",
    "    # 3. Calculate daily sentiment\n",
    "    print(\"Calculating daily sentiment...\")\n",
    "    daily = (\n",
    "        sent_df\n",
    "        .groupby(\"date\")[\"sentiment\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .assign(date=lambda df: pd.to_datetime(df[\"date\"]))\n",
    "    )\n",
    "\n",
    "    # 4. Fetch and merge prices\n",
    "    print(\"Fetching price data...\")\n",
    "    try:\n",
    "        price_df = fetch_prices(ticker)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching prices: {e}\")\n",
    "        return\n",
    "\n",
    "    merged = price_df.merge(daily, on=\"date\", how=\"left\")\n",
    "    merged[\"sentiment\"].fillna(0, inplace=True)  # Neutral for days without articles\n",
    "    merged[\"sentiment_ma\"] = merged[\"sentiment\"].rolling(7, min_periods=1).mean()\n",
    "\n",
    "    # Filter by date range if specified\n",
    "    if start_date:\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        merged = merged[merged['date'] >= start_date]\n",
    "    if end_date:\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "        merged = merged[merged['date'] <= end_date]\n",
    "\n",
    "    if merged.empty:\n",
    "        print(\"No data available for the specified date range.\")\n",
    "        return\n",
    "\n",
    "    # 5. Create plot in p2 style\n",
    "    print(\"Generating plot...\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plot stock prices\n",
    "    ax1.plot(merged['date'], merged['close'], 'b-', linewidth=2, label='Stock Price')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Stock Price', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax1.set_title(f'{ticker} Stock Price vs. News Sentiment')\n",
    "\n",
    "    # Create second axis for sentiment\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot sentiment rolling average\n",
    "    ax2.plot(merged['date'], merged['sentiment_ma'],\n",
    "            'r-', linewidth=2,\n",
    "            label='7-day Sentiment Avg')\n",
    "    ax2.set_ylabel('Sentiment Score (7-day MA)', color='r')\n",
    "    ax2.tick_params('y', colors='r')\n",
    "\n",
    "    # Add horizontal line at 0 for neutral sentiment\n",
    "    ax2.axhline(0, color='gray', linestyle='--', linewidth=0.8, label='Neutral')\n",
    "\n",
    "    # Add markers for the actual sentiment period\n",
    "    ax2.axvline(pd.to_datetime(sentiment_start), color='g', linestyle=':', alpha=0.7, label='Sentiment Start')\n",
    "    ax2.axvline(pd.to_datetime(sentiment_end), color='r', linestyle=':', alpha=0.7, label='Sentiment End')\n",
    "\n",
    "    # Combine legends from both axes\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Show sample of the most positive/negative news (from p2)\n",
    "    print(\"\\nTop 3 Positive News:\")\n",
    "    print(sent_df.nlargest(3, 'sentiment')[['date', 'title', 'sentiment']])\n",
    "\n",
    "    print(\"\\nTop 3 Negative News:\")\n",
    "    print(sent_df.nsmallest(3, 'sentiment')[['date', 'title', 'sentiment']])\n",
    "\n",
    "# ─── ENTRY POINT ──────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    ticker = input(\"Enter ticker (e.g. AAPL): \").strip().upper()\n",
    "    try:\n",
    "        # Example: main(ticker, start_date='2025-04-15', end_date='2025-07-15')\n",
    "        main(ticker)  # Use without date range for full history\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0963d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 1. Load your custom model\n",
    "model_path = \"finbert-finetuned-final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# 2. Fetch news from FMP with proper error handling\n",
    "def fetch_news(symbol, api_key, days=3730):\n",
    "    cutoff = datetime.utcnow() - timedelta(days=days)\n",
    "    all_articles = []\n",
    "    page = 0\n",
    "\n",
    "    while True:\n",
    "        url = f\"https://financialmodelingprep.com/api/v3/stock_news?symbol={symbol}&page={page}&apikey={api_key}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            news = response.json()\n",
    "\n",
    "            if not news:\n",
    "                break  # No more articles\n",
    "\n",
    "            # Filter articles by date\n",
    "            page_articles = []\n",
    "            for article in news:\n",
    "                try:\n",
    "                    article_date = datetime.strptime(article['publishedDate'], \"%Y-%m-%d %H:%M:%S\")\n",
    "                    if article_date < cutoff:\n",
    "                        continue  # Skip articles older than cutoff\n",
    "                    page_articles.append(article)\n",
    "                except (KeyError, ValueError):\n",
    "                    continue\n",
    "\n",
    "            all_articles.extend(page_articles)\n",
    "\n",
    "            # Check if we've reached the cutoff date\n",
    "            if len(page_articles) < len(news):\n",
    "                break  # This page contained articles beyond cutoff date\n",
    "\n",
    "            page += 1\n",
    "\n",
    "            # Safety limit to prevent infinite loops\n",
    "            if page > 50:  # Max 50 pages (50,000 articles)\n",
    "                print(\"Reached maximum page limit\")\n",
    "                break\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Found {len(all_articles)} articles within {days} days\")\n",
    "    return all_articles\n",
    "\n",
    "# 3. Analyze sentiment with YOUR model\n",
    "def analyze_sentiment(articles):\n",
    "    results = []\n",
    "    for art in articles:\n",
    "        try:\n",
    "            # Use title if content is missing\n",
    "            text = art.get('content', art.get('title', ''))\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            inputs = tokenizer(text,\n",
    "                              return_tensors=\"pt\",\n",
    "                              truncation=True,\n",
    "                              max_length=512)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "            probs = torch.softmax(logits, dim=1)[0]\n",
    "\n",
    "            results.append({\n",
    "                'date': datetime.strptime(art['publishedDate'], \"%Y-%m-%d %H:%M:%S\").date(),\n",
    "                'sentiment': probs[0].item() - probs[2].item(),  # positive - negative\n",
    "                'title': art['title'],\n",
    "                'source': art.get('site', 'Unknown')\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article '{art.get('title', '')}': {e}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 4. Fetch stock prices with error handling\n",
    "def fetch_prices(symbol, api_key):\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{symbol}?apikey={api_key}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if response contains historical data\n",
    "        if 'historical' not in data:\n",
    "            print(f\"No price data found: {data}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        prices = pd.DataFrame(data['historical'])\n",
    "        prices['date'] = pd.to_datetime(prices['date'])\n",
    "        return prices[['date', 'close']]\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch prices: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# 5. Main workflow\n",
    "API_KEY = \"API_KEY_HERE\"  # Replace with your actual API key\n",
    "SYMBOL = \"AAPL\"\n",
    "\n",
    "# Fetch and process data\n",
    "print(\"Fetching news...\")\n",
    "news = fetch_news(SYMBOL, API_KEY, days=3507)\n",
    "print(f\"Found {len(news)} articles\")\n",
    "\n",
    "print(\"Analyzing sentiment...\")\n",
    "sentiment_df = analyze_sentiment(news)\n",
    "print(f\"Processed {len(sentiment_df)} articles\")\n",
    "\n",
    "print(\"Fetching stock prices...\")\n",
    "prices_df = fetch_prices(SYMBOL, API_KEY)\n",
    "print(f\"Found {len(prices_df)} price records\")\n",
    "\n",
    "if sentiment_df.empty or prices_df.empty:\n",
    "    print(\"Insufficient data for visualization\")\n",
    "else:\n",
    "    # Aggregate daily sentiment\n",
    "    daily_sentiment = sentiment_df.groupby('date')['sentiment'].mean().reset_index()\n",
    "    daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])\n",
    "\n",
    "    # Merge with prices\n",
    "    merged_df = pd.merge(\n",
    "        prices_df,\n",
    "        daily_sentiment,\n",
    "        on='date',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Filter to the period where we have sentiment data\n",
    "    sentiment_start = daily_sentiment['date'].min()\n",
    "    sentiment_end = daily_sentiment['date'].max()\n",
    "    chart_start = sentiment_start - timedelta(days=30)\n",
    "    chart_end = sentiment_end + timedelta(days=5)\n",
    "\n",
    "    filtered_df = merged_df[\n",
    "        (merged_df['date'] >= chart_start) &\n",
    "        (merged_df['date'] <= chart_end)\n",
    "    ]\n",
    "\n",
    "    # Fill missing sentiment with 0 (neutral)\n",
    "    filtered_df['sentiment'].fillna(0, inplace=True)\n",
    "\n",
    "    # Add rolling average for sentiment\n",
    "    filtered_df['sentiment_ma'] = filtered_df['sentiment'].rolling(7, min_periods=1).mean()\n",
    "\n",
    "    # 6. Visualization - single plot with dual axes\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plot stock prices\n",
    "    ax1.plot(filtered_df['date'], filtered_df['close'], 'b-', linewidth=2, label='Stock Price')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Stock Price', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax1.set_title(f'{SYMBOL} Stock Price vs. News Sentiment')\n",
    "\n",
    "    # Create second axis for sentiment\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot sentiment rolling average\n",
    "    ax2.plot(filtered_df['date'], filtered_df['sentiment_ma'],\n",
    "            'r-', linewidth=2,\n",
    "            label='7-day Sentiment Avg')\n",
    "    ax2.set_ylabel('Sentiment Score (7-day MA)', color='r')\n",
    "    ax2.tick_params('y', colors='r')\n",
    "\n",
    "    # Add horizontal line at 0 for neutral sentiment\n",
    "    ax2.axhline(0, color='gray', linestyle='--', linewidth=0.8, label='Neutral')\n",
    "\n",
    "    # Add markers for the actual sentiment period\n",
    "    ax2.axvline(sentiment_start, color='g', linestyle=':', alpha=0.7, label='Sentiment Start')\n",
    "    ax2.axvline(sentiment_end, color='r', linestyle=':', alpha=0.7, label='Sentiment End')\n",
    "\n",
    "    # Combine legends from both axes\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Show sample of the most positive/negative news\n",
    "    print(\"\\nTop 3 Positive News:\")\n",
    "    print(sentiment_df.nlargest(3, 'sentiment')[['date', 'title', 'sentiment']])\n",
    "\n",
    "    print(\"\\nTop 3 Negative News:\")\n",
    "    print(sentiment_df.nsmallest(3, 'sentiment')[['date', 'title', 'sentiment']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mqs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
