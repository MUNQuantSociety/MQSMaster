{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5791ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184004c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sentiment.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ─── resolve script_dir ───────────────────────────────────────────────────────\n",
    "try:\n",
    "    script_dir = Path(__file__).parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "\n",
    "proj_root = script_dir.parent\n",
    "if str(proj_root) not in sys.path:\n",
    "    sys.path.insert(0, str(proj_root))\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "API_KEY    =  os.getenv(\"FMP_API_KEY\")   # your FMP key\n",
    "MODEL_PATH = script_dir / \"finbert-finetuned-final\"\n",
    "ART_DIR    = script_dir / \"articles\"\n",
    "CHUNK_SIZE = 32     # adjust down if you still hit OOM on GPU\n",
    "\n",
    "# ─── HELPERS ──────────────────────────────────────────────────────────────────\n",
    "\n",
    "def load_articles(ticker: str) -> pd.DataFrame:\n",
    "    path = ART_DIR / f\"{ticker}.csv\"\n",
    "    df = pd.read_csv(path, parse_dates=[\"publishedDate\"])\n",
    "    df[\"date\"] = df[\"publishedDate\"].dt.date\n",
    "    return df\n",
    "\n",
    "def analyze_sentiment_chunked(\n",
    "    df: pd.DataFrame,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device: torch.device,\n",
    "    chunk_size: int\n",
    ") -> pd.DataFrame:\n",
    "    texts = (df[\"content\"].fillna(\"\") + \" \" + df[\"title\"].fillna(\"\")).tolist()\n",
    "    dates = pd.to_datetime(df[\"date\"]).tolist()\n",
    "    records = []\n",
    "\n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        batch_texts = texts[i : i + chunk_size]\n",
    "        batch_dates = dates[i : i + chunk_size]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        scores = probs[:, 0] - probs[:, 2]\n",
    "\n",
    "        for d, s in zip(batch_dates, scores):\n",
    "            records.append({\"date\": d, \"sentiment\": float(s)})\n",
    "\n",
    "        del inputs, logits, probs\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def fetch_last_7_days(ticker: str) -> pd.DataFrame:\n",
    "    url = (\n",
    "        f\"https://financialmodelingprep.com/api/v3/\"\n",
    "        f\"historical-price-full/{ticker}\"\n",
    "        f\"?timeseries=30&apikey={API_KEY}\"\n",
    "    )\n",
    "    resp = requests.get(url); resp.raise_for_status()\n",
    "    json_resp = resp.json()\n",
    "    recs = json_resp.get(\"historical\", []) if isinstance(json_resp, dict) else []\n",
    "    df = pd.DataFrame(recs)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    return df[[\"date\", \"close\"]]\n",
    "\n",
    "# ─── MAIN ─────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main(ticker: str):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 1. Load model & tokenizer\n",
    "    t0 = time.perf_counter()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH))\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        str(MODEL_PATH),\n",
    "        torch_dtype=(torch.float16 if device.type == \"cuda\" else torch.float32)\n",
    "    )\n",
    "    model.to(device).eval()\n",
    "    print(f\"Model load time: {time.perf_counter() - t0:.2f}s\")\n",
    "\n",
    "    # 2. Load articles\n",
    "    t1 = time.perf_counter()\n",
    "    art_df = load_articles(ticker)\n",
    "    print(f\"Loaded {len(art_df)} articles in {time.perf_counter() - t1:.2f}s\")\n",
    "    if art_df.empty:\n",
    "        print(f\"No articles found for {ticker}.\")\n",
    "        return\n",
    "\n",
    "    # 3. Analyze sentiment in chunks\n",
    "    t2 = time.perf_counter()\n",
    "    sent_df = analyze_sentiment_chunked(art_df, tokenizer, model, device, CHUNK_SIZE)\n",
    "    print(f\"Sentiment analysis time: {time.perf_counter() - t2:.2f}s\")\n",
    "\n",
    "    # 4. Aggregate daily sentiment\n",
    "    daily = sent_df.groupby(\"date\", as_index=False)[\"sentiment\"].mean()\n",
    "    daily[\"date\"] = pd.to_datetime(daily[\"date\"])\n",
    "\n",
    "    # 5. Fetch last 7 days of prices\n",
    "    t3 = time.perf_counter()\n",
    "    price_df = fetch_last_7_days(ticker)\n",
    "    print(f\"Price fetch time: {time.perf_counter() - t3:.2f}s\")\n",
    "\n",
    "    # 6. Merge and compute changes\n",
    "    merged = price_df.merge(daily, on=\"date\", how=\"left\")\n",
    "    merged[\"sentiment\"] = merged[\"sentiment\"].fillna(0.0)\n",
    "\n",
    "    # price change (ΔP) and its 7‑day rolling mean\n",
    "    merged[\"price_diff\"] = merged[\"close\"].diff()\n",
    "    merged[\"price_diff_ma\"] = merged[\"price_diff\"].rolling(7, min_periods=1).mean()\n",
    "\n",
    "    # sentiment change (ΔS)\n",
    "    merged[\"sentiment_diff\"] = merged[\"sentiment\"].diff().fillna(0.0)\n",
    "\n",
    "    dates = merged[\"date\"]\n",
    "\n",
    "    # ─── PLOT 1: raw price change vs. raw sentiment change ───────────────────────\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "    ax1.plot(dates, merged[\"price_diff\"], label=\"Raw Price Change\", color=\"green\")\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"Price Change\")\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(dates, merged[\"sentiment_diff\"], label=\"Raw Sentiment Change\", color=\"red\")\n",
    "    ax2.set_ylabel(\"Sentiment Change\")\n",
    "    ax2.axhline(0, linestyle=\"--\", linewidth=0.8, color=\"gray\")\n",
    "\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1 + h2, l1 + l2, loc=\"upper left\")\n",
    "    plt.title(f\"{ticker}: Raw Daily Price Change vs. Raw Daily Sentiment Change\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ─── PLOT 2: 7‑day MA price change vs. raw sentiment change ─────────────────\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "    ax1.plot(dates, merged[\"price_diff_ma\"], label=\"7‑Day Price Change MA\", color=\"green\")\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"Price Change MA\")\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(dates, merged[\"sentiment_diff\"], label=\"Raw Sentiment Change\", color=\"red\")\n",
    "    ax2.set_ylabel(\"Sentiment Change\")\n",
    "    ax2.axhline(0, linestyle=\"--\", linewidth=0.8, color=\"gray\")\n",
    "\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1 + h2, l1 + l2, loc=\"upper left\")\n",
    "    plt.title(f\"{ticker}: 7‑Day Price Change MA vs. Raw Daily Sentiment Change\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ─── ENTRY POINT ──────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ticker = input(\"Enter ticker (e.g. AAPL): \").strip().upper()\n",
    "    main(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77842afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sentiment.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ─── resolve script_dir ───────────────────────────────────────────────────────\n",
    "try:\n",
    "    script_dir = Path(__file__).parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "\n",
    "proj_root = script_dir.parent\n",
    "if str(proj_root) not in sys.path:\n",
    "    sys.path.insert(0, str(proj_root))\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "API_KEY    = os.getenv(\"FMP_API_KEY\")  # your FMP key\n",
    "MODEL_PATH = script_dir / \"finbert-finetuned-final\"\n",
    "ART_DIR    = script_dir / \"articles\"\n",
    "\n",
    "# ─── HELPERS ──────────────────────────────────────────────────────────────────\n",
    "\n",
    "def load_articles(ticker: str) -> pd.DataFrame:\n",
    "    path = ART_DIR / f\"{ticker}.csv\"\n",
    "    df = pd.read_csv(path, parse_dates=[\"publishedDate\"])\n",
    "    df[\"date\"] = df[\"publishedDate\"].dt.date\n",
    "    return df\n",
    "\n",
    "def analyze_sentiment_batched(df: pd.DataFrame, tokenizer, model, device) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tokenize all texts in one batch and do a single forward pass on the given device.\n",
    "    Sentiment = positive_prob − negative_prob.\n",
    "    \"\"\"\n",
    "    texts = (df[\"content\"].fillna(\"\") + \" \" + df[\"title\"].fillna(\"\")).tolist()\n",
    "    dates = pd.to_datetime(df[\"date\"]).tolist()\n",
    "\n",
    "    # batch-tokenize\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    scores = probs[:, 0] - probs[:, 2]\n",
    "\n",
    "    return pd.DataFrame({ \"date\": dates, \"sentiment\": scores })\n",
    "\n",
    "def fetch_last_7_days(ticker: str) -> pd.DataFrame:\n",
    "    url = (\n",
    "        f\"https://financialmodelingprep.com/api/v3/\"\n",
    "        f\"historical-price-full/{ticker}\"\n",
    "        f\"?timeseries=7&apikey={API_KEY}\"\n",
    "    )\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    json_resp = resp.json()\n",
    "    records = json_resp.get(\"historical\", []) if isinstance(json_resp, dict) else []\n",
    "    prices = pd.DataFrame(records)\n",
    "    prices[\"date\"] = pd.to_datetime(prices[\"date\"])\n",
    "    return prices[[\"date\", \"close\"]]\n",
    "\n",
    "# ─── MAIN ─────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main(ticker: str):\n",
    "    # pick GPU if you have one\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 1. Load model & tokenizer\n",
    "    t0 = time.perf_counter()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH))\n",
    "    model     = AutoModelForSequenceClassification.from_pretrained(str(MODEL_PATH))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model load time: {time.perf_counter() - t0:.2f}s\")\n",
    "\n",
    "    # 2. Load articles\n",
    "    t1 = time.perf_counter()\n",
    "    art_df = load_articles(ticker)\n",
    "    print(f\"Loaded {len(art_df)} articles in {time.perf_counter() - t1:.2f}s\")\n",
    "\n",
    "    if art_df.empty:\n",
    "        print(f\"No articles found for {ticker}.\")\n",
    "        return\n",
    "\n",
    "    # 3. Analyze sentiment (batched on GPU)\n",
    "    t2 = time.perf_counter()\n",
    "    sent_df = analyze_sentiment_batched(art_df, tokenizer, model, device)\n",
    "    print(f\"Sentiment analysis time: {time.perf_counter() - t2:.2f}s\")\n",
    "\n",
    "    # 4. Aggregate daily\n",
    "    daily = sent_df.groupby(\"date\", as_index=False)[\"sentiment\"].mean()\n",
    "    daily[\"date\"] = pd.to_datetime(daily[\"date\"])\n",
    "\n",
    "    # 5. Fetch last 7 days of prices\n",
    "    t3 = time.perf_counter()\n",
    "    price_df = fetch_last_7_days(ticker)\n",
    "    print(f\"Price fetch time: {time.perf_counter() - t3:.2f}s\")\n",
    "\n",
    "    # 6. Merge and fill missing sentiment (no chained assignment)\n",
    "    merged = price_df.merge(daily, on=\"date\", how=\"left\")\n",
    "    merged[\"sentiment\"] = merged[\"sentiment\"].fillna(0)\n",
    "    merged[\"sentiment_ma\"] = merged[\"sentiment\"].rolling(7, min_periods=1).mean()\n",
    "\n",
    "    # 7. Plot with green price / red sentiment\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "    ax1.plot(merged[\"date\"], merged[\"close\"], label=\"Close Price\", color=\"green\")\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"Price\")\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(\n",
    "        merged[\"date\"],\n",
    "        merged[\"sentiment_ma\"],\n",
    "        label=\"7-Day Sentiment MA\",\n",
    "        color=\"red\"\n",
    "    )\n",
    "    ax2.set_ylabel(\"Sentiment Score\")\n",
    "    ax2.axhline(0, linestyle=\"--\", linewidth=0.8, color=\"gray\")\n",
    "\n",
    "    # combine legends\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1 + h2, l1 + l2, loc=\"upper left\")\n",
    "\n",
    "    plt.title(f\"{ticker}: Price vs. Sentiment (Last 7 Days)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ─── ENTRY POINT ──────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ticker = input(\"Enter ticker (e.g. AAPL): \").strip().upper()\n",
    "    main(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51fee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 1: Compute & Save Per-Article & Daily Sentiment Scores ───────────────\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "ART_DIR    = Path(\"articles\")\n",
    "MODEL_DIR  = Path(\"finbert-finetuned-final\")\n",
    "OUT_DIR    = Path(\"sentiment_scores\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "CHUNK_SIZE = 32  # reduce if you still hit OOM\n",
    "\n",
    "# ─── USER INPUT ───────────────────────────────────────────────────────────────\n",
    "tickers_input = input(\"Enter ticker symbols (comma-separated, e.g. AAPL,MSFT): \")\n",
    "TICKERS = [t.strip().upper() for t in tickers_input.split(\",\") if t.strip()]\n",
    "if not TICKERS:\n",
    "    raise ValueError(\"Please enter at least one ticker symbol.\")\n",
    "\n",
    "print(f\"Will compute sentiment for: {TICKERS}\")\n",
    "\n",
    "# ─── DEVICE & MODEL ───────────────────────────────────────────────────────────\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model     = (\n",
    "    AutoModelForSequenceClassification\n",
    "    .from_pretrained(MODEL_DIR, torch_dtype=(torch.float16 if device.type==\"cuda\" else torch.float32))\n",
    "    .to(device)\n",
    "    .eval()\n",
    ")\n",
    "\n",
    "# ─── PROCESS EACH TICKER ──────────────────────────────────────────────────────\n",
    "for ticker in tqdm(TICKERS, desc=\"Computing sentiment\"):\n",
    "    # Load and prepare texts\n",
    "    try:\n",
    "        df = pd.read_csv(ART_DIR/f\"{ticker}.csv\", parse_dates=[\"publishedDate\"])\n",
    "    except FileNotFoundError:\n",
    "        tqdm.write(f\"Warning: Article file not found for {ticker}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    df[\"date\"] = df[\"publishedDate\"].dt.date\n",
    "    texts      = (df[\"content\"].fillna(\"\") + \" \" + df[\"title\"].fillna(\"\")).tolist()\n",
    "    dates      = pd.to_datetime(df[\"date\"]).tolist()\n",
    "\n",
    "    records = []\n",
    "    # Chunked inference\n",
    "    for i in range(0, len(texts), CHUNK_SIZE):\n",
    "        batch_texts = texts[i : i + CHUNK_SIZE]\n",
    "        batch_dates = dates[i : i + CHUNK_SIZE]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "\n",
    "        probs  = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        scores = probs[:,0] - probs[:,2] # Positive probability - Negative probability\n",
    "\n",
    "        records.extend(zip(batch_dates, scores))\n",
    "        del inputs, logits, probs\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Build DataFrame\n",
    "    out   = pd.DataFrame(records, columns=[\"date\", \"sentiment\"])\n",
    "    daily = out.groupby(\"date\", as_index=False)[\"sentiment\"].mean()\n",
    "\n",
    "    # Save per-article and per-day CSVs\n",
    "    out.to_csv(OUT_DIR/f\"{ticker}_article_scores.csv\", index=False)\n",
    "    daily.to_csv(OUT_DIR/f\"{ticker}_daily_scores.csv\",  index=False)\n",
    "\n",
    "    tqdm.write(f\"{ticker}: saved {len(out)} article rows and {len(daily)} daily rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96deffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 2: Fetch Stock & Market Prices ──────────────────────────────────────\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "API_KEY     = os.getenv(\"FMP_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"FMP_API_KEY environment variable not set.\")\n",
    "    \n",
    "PRICE_DIR   = Path(\"stock_price\")\n",
    "SENT_DIR    = Path(\"sentiment_scores\")\n",
    "PRICE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ─── USER INPUT ───────────────────────────────────────────────────────────────\n",
    "tickers_input = input(\"Enter tickers (e.g. AAPL,MSFT) or 'ALL' for all from sentiment folder: \")\n",
    "if tickers_input.strip().upper() == \"ALL\":\n",
    "    TICKERS = [p.stem.replace(\"_daily_scores\", \"\") for p in SENT_DIR.glob(\"*_daily_scores.csv\")]\n",
    "    print(f\"Found all available tickers: {TICKERS}\")\n",
    "else:\n",
    "    TICKERS = [t.strip().upper() for t in tickers_input.split(\",\") if t.strip()]\n",
    "\n",
    "if not TICKERS:\n",
    "    raise ValueError(\"No tickers specified.\")\n",
    "\n",
    "# Also fetch the market index (S&P 500 ETF) for CAPM calculation\n",
    "tickers_to_fetch = list(set(TICKERS + [\"SPY\"]))\n",
    "\n",
    "def fetch_and_save(ticker: str, days: int = 365 * 2):\n",
    "    \"\"\"Fetches historical closing prices and saves them to a CSV.\"\"\"\n",
    "    print(f\"Fetching prices for {ticker}...\")\n",
    "    url = (\n",
    "        f\"https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}\"\n",
    "        f\"?timeseries={days}&apikey={API_KEY}\"\n",
    "    )\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json().get(\"historical\", [])\n",
    "    if not data:\n",
    "        print(f\"Warning: No price data found for {ticker}. Skipping.\")\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df[[\"date\", \"close\"]].sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    df.to_csv(PRICE_DIR/f\"{ticker}_price.csv\", index=False)\n",
    "    print(f\"  > Saved {len(df)} rows of price data for {ticker}\")\n",
    "\n",
    "for t in tqdm(tickers_to_fetch, desc=\"Fetching prices\"):\n",
    "    fetch_and_save(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df63348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 3: Calculate CAPM and Alpha ─────────────────────────────────────────\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "PRICE_DIR = Path(\"stock_price\")\n",
    "CAPM_DIR  = Path(\"CAPM\")\n",
    "CAPM_DIR.mkdir(exist_ok=True)\n",
    "ANNUAL_RISK_FREE_RATE = 0.02\n",
    "DAILY_RISK_FREE_RATE  = ANNUAL_RISK_FREE_RATE / 252\n",
    "BETA_WINDOW           = 60\n",
    "\n",
    "# ─── USER INPUT ───────────────────────────────────────────────────────────────\n",
    "tickers_input = input(\"Enter tickers (e.g. AAPL,MSFT) or 'ALL' for all from price folder: \")\n",
    "if tickers_input.strip().upper() == \"ALL\":\n",
    "    TICKERS = [p.stem.replace(\"_price\", \"\") for p in PRICE_DIR.glob(\"*_price.csv\") if \"SPY\" not in p.stem]\n",
    "    print(f\"Found all available tickers: {TICKERS}\")\n",
    "else:\n",
    "    TICKERS = [t.strip().upper() for t in tickers_input.split(\",\") if t.strip()]\n",
    "\n",
    "if not TICKERS:\n",
    "    raise ValueError(\"No tickers specified.\")\n",
    "\n",
    "# ─── LOAD MARKET DATA ─────────────────────────────────────────────────────────\n",
    "try:\n",
    "    market_df = pd.read_csv(PRICE_DIR/\"SPY_price.csv\", parse_dates=[\"date\"])\n",
    "    market_df[\"market_return\"] = market_df[\"close\"].pct_change()\n",
    "except FileNotFoundError:\n",
    "    raise SystemExit(\"SPY_price.csv not found. Please run Cell 2 first.\")\n",
    "    \n",
    "# ─── CALCULATE ALPHA FOR EACH TICKER ──────────────────────────────────────────\n",
    "for ticker in tqdm(TICKERS, desc=\"Calculating Alpha (CAPM)\"):\n",
    "    try:\n",
    "        stock_df = pd.read_csv(PRICE_DIR/f\"{ticker}_price.csv\", parse_dates=[\"date\"])\n",
    "        stock_df[\"stock_return\"] = stock_df[\"close\"].pct_change()\n",
    "    except FileNotFoundError:\n",
    "        tqdm.write(f\"Price file for {ticker} not found. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    df = pd.merge(stock_df[[\"date\", \"stock_return\"]], market_df[[\"date\", \"market_return\"]], on=\"date\", how=\"inner\")\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    rolling_cov = df['stock_return'].rolling(window=BETA_WINDOW).cov(df['market_return'])\n",
    "    rolling_var = df['market_return'].rolling(window=BETA_WINDOW).var()\n",
    "    df['beta']  = rolling_cov / rolling_var\n",
    "\n",
    "    market_risk_premium = df['market_return'] - DAILY_RISK_FREE_RATE\n",
    "    df['expected_return'] = DAILY_RISK_FREE_RATE + (df['beta'] * market_risk_premium)\n",
    "    df['alpha'] = df['stock_return'] - df['expected_return']\n",
    "\n",
    "    output_df = df[[\"date\", \"alpha\", \"beta\", \"stock_return\", \"expected_return\"]].dropna()\n",
    "    output_df.to_csv(CAPM_DIR/f\"{ticker}_CAPM.csv\", index=False)\n",
    "    tqdm.write(f\"{ticker}: calculated and saved {len(output_df)} rows of CAPM data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 4: Plotting with Raw Per-Article Data ───────────────────────────────\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "SENT_DIR  = Path(\"sentiment_scores\")\n",
    "PRICE_DIR = Path(\"stock_price\")\n",
    "CAPM_DIR  = Path(\"CAPM\")\n",
    "ROLLING_AVG = 7\n",
    "\n",
    "# ─── USER INPUT ───────────────────────────────────────────────────────────────\n",
    "tickers_input = input(\"Enter tickers (e.g. AAPL,MSFT) or 'ALL' for all from CAPM folder: \")\n",
    "if tickers_input.strip().upper() == 'ALL':\n",
    "    TICKERS = [p.stem.replace(\"_CAPM\", \"\") for p in CAPM_DIR.glob(\"*.csv\")]\n",
    "    print(f\"Found all available tickers: {TICKERS}\")\n",
    "else:\n",
    "    TICKERS = [t.strip().upper() for t in tickers_input.split(\",\") if t.strip()]\n",
    "\n",
    "if not TICKERS:\n",
    "    raise ValueError(\"No tickers specified.\")\n",
    "\n",
    "# ─── GENERATE PLOTS ───────────────────────────────────────────────────────────\n",
    "for ticker in TICKERS:\n",
    "    try:\n",
    "        # Data for Plot 1 (Alpha vs Daily Sentiment)\n",
    "        daily_sent_df = pd.read_csv(SENT_DIR/f\"{ticker}_daily_scores.csv\", parse_dates=[\"date\"])\n",
    "        capm_df  = pd.read_csv(CAPM_DIR/f\"{ticker}_CAPM.csv\", parse_dates=[\"date\"])\n",
    "        \n",
    "        # Data for Plot 2 (Raw Price vs Per-Article Sentiment)\n",
    "        article_sent_df = pd.read_csv(SENT_DIR/f\"{ticker}_article_scores.csv\", parse_dates=[\"date\"])\n",
    "        price_df = pd.read_csv(PRICE_DIR/f\"{ticker}_price.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Could not find a required file for {ticker}: {e}. Skipping plots.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Generating plots for {ticker} ---\")\n",
    "\n",
    "    # ─── PLOT 1: ALPHA vs DAILY SENTIMENT (Unchanged) ─────────────────────────\n",
    "    # Merge data for the first plot\n",
    "    df1 = pd.merge(price_df, daily_sent_df, on=\"date\", how=\"left\")\n",
    "    df1 = pd.merge(df1, capm_df, on=\"date\", how=\"left\")\n",
    "\n",
    "    if not df1.empty:\n",
    "        df1 = df1.sort_values('date')\n",
    "        last_date = df1['date'].max()\n",
    "        one_year_ago = last_date - pd.DateOffset(years=1)\n",
    "        df1 = df1[df1['date'] >= one_year_ago].copy()\n",
    "\n",
    "    df1.set_index('date', inplace=True)\n",
    "    df1[['sentiment', 'alpha']] = df1[['sentiment', 'alpha']].fillna(method='ffill')\n",
    "    df1.reset_index(inplace=True)\n",
    "    df1.dropna(subset=['close'], inplace=True)\n",
    "    df1[\"sentiment_ma\"] = df1[\"sentiment\"].rolling(ROLLING_AVG, min_periods=1).mean()\n",
    "    df1[\"alpha_ma\"] = df1[\"alpha\"].rolling(ROLLING_AVG, min_periods=1).mean()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "    plt.title(f\"{ticker}: 1-Year View of Alpha (Performance) vs. Daily Sentiment\", fontsize=16)\n",
    "    ax1.plot(df1[\"date\"], df1[\"alpha_ma\"], color=\"blue\", label=f\"{ROLLING_AVG}-Day Alpha MA\")\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"Alpha (Outperformance)\", color=\"blue\")\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    ax1.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df1[\"date\"], df1[\"sentiment_ma\"], color=\"purple\", label=f\"{ROLLING_AVG}-Day Sentiment MA\", alpha=0.8)\n",
    "    ax2.set_ylabel(\"Daily Avg. Sentiment Score\", color=\"purple\")\n",
    "    ax2.tick_params(axis='y', labelcolor='purple')\n",
    "    y1_min, y1_max = ax1.get_ylim(); y1_abs_max = max(abs(y1_min), abs(y1_max)) * 1.1; ax1.set_ylim(-y1_abs_max, y1_abs_max)\n",
    "    y2_min, y2_max = ax2.get_ylim(); y2_abs_max = max(abs(y2_min), abs(y2_max)) * 1.1; ax2.set_ylim(-y2_abs_max, y2_abs_max)\n",
    "    fig.legend(loc=\"upper left\", bbox_to_anchor=(0.1, 0.9)); plt.tight_layout(rect=[0, 0, 1, 0.96]); plt.show()\n",
    "\n",
    "    # ─── PLOT 2: RAW PRICE vs PER-ARTICLE SENTIMENT (New Logic) ───────────────\n",
    "    # Merge per-article sentiment with price data. This automatically sets the date range.\n",
    "    df2 = pd.merge(article_sent_df, price_df, on=\"date\", how=\"left\").dropna()\n",
    "\n",
    "    if df2.empty:\n",
    "        print(f\"No matching data found for per-article plot for {ticker}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "    plt.title(f\"{ticker}: Raw Stock Price vs. Per-Article Sentiment Score\", fontsize=16)\n",
    "\n",
    "    # Plot 1: Raw Price\n",
    "    ax1.plot(df2[\"date\"], df2[\"close\"], color=\"green\", label=\"Raw Price\")\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"Stock Price ($)\", color=\"green\")\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    \n",
    "    # Plot 2: Per-Article Sentiment as a scatter plot\n",
    "    ax2 = ax1.twinx()\n",
    "    # Use a scatter plot because there can be multiple articles (and scores) per day\n",
    "    ax2.scatter(df2[\"date\"], df2[\"sentiment\"], color=\"red\", label=\"Per-Article Sentiment\", alpha=0.5, s=15) # s is marker size\n",
    "    ax2.set_ylabel(\"Sentiment Score\", color=\"red\")\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    # Set sentiment limits from -1 to 1, which is the natural range of the score\n",
    "    ax2.set_ylim(-1.05, 1.05)\n",
    "    ax2.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "    \n",
    "    # Create a combined legend\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1+h2, l1+l2, loc=\"upper left\")\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    # ─── PLOT: Alpha MA vs Sentiment MA ───────────────────────────────────────\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6893fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 4: Plotting with Aligned & Synced Data ──────────────────────────────\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "SENT_DIR  = Path(\"sentiment_scores\")\n",
    "PRICE_DIR = Path(\"stock_price\")\n",
    "CAPM_DIR  = Path(\"CAPM\")\n",
    "ROLLING_AVG = 7\n",
    "\n",
    "# ─── USER INPUT ───────────────────────────────────────────────────────────────\n",
    "tickers_input = input(\"Enter tickers (e.g. AAPL,MSFT) or 'ALL' for all from CAPM folder: \")\n",
    "if tickers_input.strip().upper() == 'ALL':\n",
    "    TICKERS = [p.stem.replace(\"_CAPM\", \"\") for p in CAPM_DIR.glob(\"*.csv\")]\n",
    "    print(f\"Found all available tickers: {TICKERS}\")\n",
    "else:\n",
    "    TICKERS = [t.strip().upper() for t in tickers_input.split(\",\") if t.strip()]\n",
    "\n",
    "if not TICKERS:\n",
    "    raise ValueError(\"No tickers specified.\")\n",
    "\n",
    "# ─── GENERATE PLOTS ───────────────────────────────────────────────────────────\n",
    "for ticker in TICKERS:\n",
    "    try:\n",
    "        sent_df  = pd.read_csv(SENT_DIR/f\"{ticker}_daily_scores.csv\", parse_dates=[\"date\"])\n",
    "        price_df = pd.read_csv(PRICE_DIR/f\"{ticker}_price.csv\", parse_dates=[\"date\"])\n",
    "        capm_df  = pd.read_csv(CAPM_DIR/f\"{ticker}_CAPM.csv\", parse_dates=[\"date\"])\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Could not find a required file for {ticker}: {e}. Skipping plots.\")\n",
    "        continue\n",
    "\n",
    "    # Merge all data sources\n",
    "    df = pd.merge(price_df, sent_df, on=\"date\", how=\"left\")\n",
    "    df = pd.merge(df, capm_df, on=\"date\", how=\"left\")\n",
    "\n",
    "    # Filter data to the last 1 year\n",
    "    if not df.empty:\n",
    "        df = df.sort_values('date')\n",
    "        last_date = df['date'].max()\n",
    "        one_year_ago = last_date - pd.DateOffset(years=1)\n",
    "        df = df[df['date'] >= one_year_ago].copy()\n",
    "    \n",
    "    # Continue processing with the filtered 1-year data\n",
    "    df.set_index('date', inplace=True)\n",
    "    df[['sentiment', 'alpha']] = df[['sentiment', 'alpha']].fillna(method='ffill')\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    df[\"sentiment_ma\"] = df[\"sentiment\"].rolling(ROLLING_AVG, min_periods=1).mean()\n",
    "    df[\"alpha_ma\"]     = df[\"alpha\"].rolling(ROLLING_AVG, min_periods=1).mean()\n",
    "\n",
    "    # ❗ KEY FIX: Drop rows where either moving average is NaN to sync their date ranges\n",
    "    df.dropna(subset=['alpha_ma', 'sentiment_ma'], inplace=True)\n",
    "\n",
    "    dates = df[\"date\"]\n",
    "    print(f\"\\n--- Generating plot for {ticker} ---\")\n",
    "\n",
    "    # ─── PLOT: Alpha MA vs Sentiment MA ───────────────────────────────────────\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "    # Corrected title\n",
    "    plt.title(f\"{ticker}: 7-Day MA of Alpha vs. 7-Day MA of Sentiment\")\n",
    "    \n",
    "    # Corrected plot data and labels\n",
    "    ax1.plot(dates, df[\"alpha_ma\"], color=\"green\", label=\"Alpha (7-Day MA)\")\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"Alpha\", color=\"green\")\n",
    "    ax1.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(dates, df[\"sentiment_ma\"], color=\"red\", linestyle='dotted', label=\"Sentiment (7-Day MA)\")\n",
    "    ax2.set_ylabel(\"Sentiment Score\", color=\"red\")\n",
    "    \n",
    "    # Align the zero-lines by making the axes symmetric\n",
    "    y1_min, y1_max = ax1.get_ylim()\n",
    "    y1_abs_max = max(abs(y1_min), abs(y1_max)) * 1.1 # Add 10% padding\n",
    "    ax1.set_ylim(-y1_abs_max, y1_abs_max)\n",
    "\n",
    "    y2_min, y2_max = ax2.get_ylim()\n",
    "    y2_abs_max = max(abs(y2_min), abs(y2_max)) * 1.1 # Add 10% padding\n",
    "    ax2.set_ylim(-y2_abs_max, y2_abs_max)\n",
    "    \n",
    "    fig.legend(loc=\"upper left\", bbox_to_anchor=(0.1, 0.9))\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mqs (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
