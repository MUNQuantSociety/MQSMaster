{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83fb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.3.3)\n",
      "Collecting torch\n",
      "  Using cached torch-2.8.0-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: transformers in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.57.1)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.7-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.4.0)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: xxhash in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.13.0-cp313-cp313-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\simanta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Using cached torch-2.8.0-cp313-cp313-win_amd64.whl (241.3 MB)\n",
      "Using cached datasets-4.2.0-py3-none-any.whl (506 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "Using cached matplotlib-3.10.7-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "Downloading kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
      "Using cached aiohttp-3.13.0-cp313-cp313-win_amd64.whl (450 kB)\n",
      "Installing collected packages: torch, scikit-learn, matplotlib, kagglehub, httpx, aiohttp, datasets\n",
      "\n",
      "   ---------------------------------------- 0/7 [torch]\n",
      "   ---------------------------------------- 0/7 [torch]\n",
      "   ---------------------------------------- 0/7 [torch]\n",
      "   ---------------------------------------- 0/7 [torch]\n",
      "   ---------------------------------------- 0/7 [torch]\n",
      "   ---------------------------------------- 0/7 [torch]\n",
      "   ---------------------------------------- 0/7 [torch]\n",
      "   ---------------------------------------- 0/7 [torch]\n",
      "   ---------------------------------------- 0/7 [torch]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Simanta\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python313\\\\site-packages\\\\torch\\\\include\\\\ATen\\\\native\\\\transformers\\\\cuda\\\\mem_eff_attention\\\\iterators\\\\predicated_tile_access_iterator_residual_last.h'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install required packages\n",
    "!pip install pandas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5fcbff3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ================================================================\n",
    "# STEP 1: Merge CSV Files for Ticker\n",
    "# ================================================================\n",
    "\n",
    "def merge_csv_files(ticker_symbol, base_path='', output_file=None):\n",
    "    \"\"\"\n",
    "    Merge three CSV files:\n",
    "    - {ticker}.csv: publishedDate, title, content, site\n",
    "    - {ticker}_article_scores.csv: timestamp, sentiment, title, site\n",
    "    - {ticker}_CAPM.csv: date, alpha, beta, stock_return, expected_return\n",
    "    \"\"\"\n",
    "    if output_file is None:\n",
    "        output_file = f'{ticker_symbol}_merged.csv'\n",
    "    \n",
    "    article_file = os.path.join(base_path, f'{ticker_symbol}.csv')\n",
    "    score_file = os.path.join(base_path, f'{ticker_symbol}_article_scores.csv')\n",
    "    capm_file = os.path.join(base_path, f'{ticker_symbol}_CAPM.csv')\n",
    "    \n",
    "    print(f\"Loading files for {ticker_symbol}...\")\n",
    "    \n",
    "    try:\n",
    "        df_articles = pd.read_csv(article_file)\n",
    "        df_scores = pd.read_csv(score_file)\n",
    "        df_capm = pd.read_csv(capm_file)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Initial shapes: Articles={df_articles.shape}, Scores={df_scores.shape}, CAPM={df_capm.shape}\")\n",
    "    \n",
    "    # Extract date columns\n",
    "    article_date_col = 'publishedDate' if 'publishedDate' in df_articles.columns else df_articles.columns[0]\n",
    "    score_date_col = 'timestamp' if 'timestamp' in df_scores.columns else df_scores.columns[0]\n",
    "    capm_date_col = 'date' if 'date' in df_capm.columns else df_capm.columns[0]\n",
    "    \n",
    "    df_articles['date'] = pd.to_datetime(df_articles[article_date_col], errors='coerce')\n",
    "    df_scores['date'] = pd.to_datetime(df_scores[score_date_col], errors='coerce')\n",
    "    df_capm['date'] = pd.to_datetime(df_capm[capm_date_col], errors='coerce')\n",
    "    \n",
    "    # Extract relevant columns\n",
    "    article_col = 'content' if 'content' in df_articles.columns else 'title'\n",
    "    sentiment_col = 'sentiment' if 'sentiment' in df_scores.columns else df_scores.select_dtypes(include=[np.number]).columns[0]\n",
    "    alpha_col = 'alpha' if 'alpha' in df_capm.columns else df_capm.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    # Prepare clean dataframes\n",
    "    df_articles_clean = df_articles[['date', article_col]].copy()\n",
    "    df_articles_clean.columns = ['date', 'article']\n",
    "    \n",
    "    df_scores_clean = df_scores[['date', sentiment_col]].copy()\n",
    "    df_scores_clean.columns = ['date', 'sentiment_score']\n",
    "    \n",
    "    df_capm_clean = df_capm[['date', alpha_col]].copy()\n",
    "    df_capm_clean.columns = ['date', 'alpha']\n",
    "    \n",
    "    # Drop invalid dates\n",
    "    df_articles_clean = df_articles_clean.dropna(subset=['date'])\n",
    "    df_scores_clean = df_scores_clean.dropna(subset=['date'])\n",
    "    df_capm_clean = df_capm_clean.dropna(subset=['date'])\n",
    "    \n",
    "    # Convert to date only (no time)\n",
    "    df_articles_clean['date'] = df_articles_clean['date'].dt.date\n",
    "    df_scores_clean['date'] = df_scores_clean['date'].dt.date\n",
    "    df_capm_clean['date'] = df_capm_clean['date'].dt.date\n",
    "    \n",
    "    # Aggregate by date\n",
    "    df_articles_agg = df_articles_clean.groupby('date')['article'].apply(\n",
    "        lambda x: ' '.join(x.dropna().astype(str))\n",
    "    ).reset_index()\n",
    "    df_scores_agg = df_scores_clean.groupby('date')['sentiment_score'].mean().reset_index()\n",
    "    df_capm_agg = df_capm_clean.groupby('date')['alpha'].mean().reset_index()\n",
    "    \n",
    "    # Merge\n",
    "    merged = df_articles_agg.merge(df_scores_agg, on='date', how='inner')\n",
    "    merged = merged.merge(df_capm_agg, on='date', how='inner')\n",
    "    merged = merged.dropna()\n",
    "    merged = merged[merged['article'].str.strip().str.len() > 0]\n",
    "    \n",
    "    print(f\"Merged: {merged.shape[0]} samples from {merged['date'].min()} to {merged['date'].max()}\")\n",
    "    \n",
    "    output_path = os.path.join(base_path, output_file)\n",
    "    merged.to_csv(output_path, index=False)\n",
    "    print(f\"Saved to {output_path}\\n\")\n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# STEP 2: Correlation Agent (Policy Network)\n",
    "# ================================================================\n",
    "\n",
    "class CorrelationAgent:\n",
    "    \"\"\"\n",
    "    Agent that learns sentiment-alpha correlation through:\n",
    "    1. Supervised Learning\n",
    "    2. Evolutionary Fine-tuning\n",
    "    3. RL Bandit-style Fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"yiyanghkust/finbert-tone\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load base transformer\n",
    "        base_model = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        \n",
    "        # Build policy network with dual heads\n",
    "        self.policy_network = nn.ModuleDict({\n",
    "            'encoder': base_model,\n",
    "            'sentiment_head': nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_size, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(256, 1),\n",
    "                nn.Tanh()  # [-1, 1]\n",
    "            ),\n",
    "            'alpha_head': nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_size, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(256, 1)\n",
    "            )\n",
    "        }).to(self.device)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.policy_network['encoder'](input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]\n",
    "        sentiment_pred = self.policy_network['sentiment_head'](pooled).squeeze(-1)\n",
    "        alpha_pred = self.policy_network['alpha_head'](pooled).squeeze(-1)\n",
    "        return sentiment_pred, alpha_pred\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, text: str):\n",
    "        \"\"\"Predict sentiment and alpha for a given text\"\"\"\n",
    "        self.policy_network.eval()\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        sentiment, alpha = self.forward(inputs['input_ids'], inputs['attention_mask'])\n",
    "        return sentiment.item(), alpha.item()\n",
    "    \n",
    "    # Parameter manipulation for evolution\n",
    "    def get_parameters(self, mode=\"head\"):\n",
    "        \"\"\"Extract parameters for evolution\"\"\"\n",
    "        parts = []\n",
    "        for name, param in self.policy_network.named_parameters():\n",
    "            if mode == \"head\" and (\"sentiment_head\" in name or \"alpha_head\" in name):\n",
    "                parts.append(param.data.detach().cpu().numpy().astype(np.float32).ravel())\n",
    "            elif mode == \"all\":\n",
    "                parts.append(param.data.detach().cpu().numpy().astype(np.float32).ravel())\n",
    "        return np.concatenate(parts) if parts else np.array([], dtype=np.float32)\n",
    "    \n",
    "    def set_parameters(self, flat_params, mode=\"head\"):\n",
    "        \"\"\"Set parameters from flat array\"\"\"\n",
    "        pointer = 0\n",
    "        for name, param in self.policy_network.named_parameters():\n",
    "            should_set = False\n",
    "            if mode == \"head\" and (\"sentiment_head\" in name or \"alpha_head\" in name):\n",
    "                should_set = True\n",
    "            elif mode == \"all\":\n",
    "                should_set = True\n",
    "            \n",
    "            if should_set:\n",
    "                shape = tuple(param.data.shape)\n",
    "                size = int(np.prod(shape))\n",
    "                chunk = flat_params[pointer:pointer+size]\n",
    "                pointer += size\n",
    "                values = chunk.reshape(shape).astype(np.float32)\n",
    "                with torch.no_grad():\n",
    "                    param.data.copy_(torch.from_numpy(values).to(param.data.dtype).to(self.device))\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate_fitness(self, dataloader):\n",
    "        \"\"\"Fitness = negative correlation loss (maximize correlation)\"\"\"\n",
    "        self.policy_network.eval()\n",
    "        all_sent_preds, all_alpha_preds = [], []\n",
    "        all_sent_true, all_alpha_true = [], []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            sentiment_pred, alpha_pred = self.forward(batch['input_ids'], batch['attention_mask'])\n",
    "            \n",
    "            all_sent_preds.extend(sentiment_pred.cpu().numpy())\n",
    "            all_alpha_preds.extend(alpha_pred.cpu().numpy())\n",
    "            all_sent_true.extend(batch['sentiment_score'].cpu().numpy())\n",
    "            all_alpha_true.extend(batch['alpha'].cpu().numpy())\n",
    "        \n",
    "        # Fitness = correlation between predicted sentiment and true alpha\n",
    "        corr = np.corrcoef(all_sent_preds, all_alpha_true)[0, 1]\n",
    "        return float(corr) if not np.isnan(corr) else 0.0\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# STEP 3: Dataset\n",
    "# ================================================================\n",
    "\n",
    "class CorrelationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            row['article'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'sentiment_score': torch.tensor(row['sentiment_score'], dtype=torch.float32),\n",
    "            'alpha': torch.tensor(row['alpha'], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# STEP 4: Three-Phase Training Pipeline\n",
    "# ================================================================\n",
    "\n",
    "def train_hybrid_agent(ticker_symbol, base_path='', model_name='yiyanghkust/finbert-tone'):\n",
    "    \"\"\"\n",
    "    Three-phase hybrid training:\n",
    "    1. Supervised Learning\n",
    "    2. Evolutionary Fine-tuning\n",
    "    3. RL Bandit Fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load merged data\n",
    "    merged_path = os.path.join(base_path, f'{ticker_symbol}_merged.csv')\n",
    "    df = pd.read_csv(merged_path)\n",
    "    \n",
    "    # Normalize\n",
    "    if df['sentiment_score'].min() >= 0:\n",
    "        df['sentiment_score'] = (df['sentiment_score'] - 0.5) * 2\n",
    "    df['alpha'] = (df['alpha'] - df['alpha'].mean()) / df['alpha'].std()\n",
    "    \n",
    "    # Split\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Data split: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # PHASE 1: SUPERVISED LEARNING\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 1: SUPERVISED LEARNING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    agent = CorrelationAgent(model_name=model_name)\n",
    "    \n",
    "    train_dataset = CorrelationDataset(train_df, agent.tokenizer)\n",
    "    val_dataset = CorrelationDataset(val_df, agent.tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    optimizer = AdamW(agent.policy_network.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    sl_epochs = 3\n",
    "    \n",
    "    for epoch in range(sl_epochs):\n",
    "        agent.policy_network.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(agent.device) for k, v in batch.items()}\n",
    "            sentiment_pred, alpha_pred = agent.forward(batch['input_ids'], batch['attention_mask'])\n",
    "            \n",
    "            # Multi-objective loss\n",
    "            loss_sent = F.mse_loss(sentiment_pred, batch['sentiment_score'])\n",
    "            loss_alpha = F.mse_loss(alpha_pred, batch['alpha'])\n",
    "            \n",
    "            # Correlation loss\n",
    "            sent_norm = (sentiment_pred - sentiment_pred.mean()) / (sentiment_pred.std() + 1e-8)\n",
    "            alpha_true_norm = (batch['alpha'] - batch['alpha'].mean()) / (batch['alpha'].std() + 1e-8)\n",
    "            corr_loss = -(sent_norm * alpha_true_norm).mean()\n",
    "            \n",
    "            loss = 0.3 * loss_sent + 0.3 * loss_alpha + 0.4 * corr_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(agent.policy_network.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        val_fitness = agent.evaluate_fitness(val_loader)\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{sl_epochs}: Train Loss={avg_train_loss:.4f}, Val Corr={val_fitness:.4f}\")\n",
    "        \n",
    "        if avg_train_loss < best_val_loss:\n",
    "            best_val_loss = avg_train_loss\n",
    "            torch.save(agent.policy_network.state_dict(), f'{ticker_symbol}_supervised.pt')\n",
    "    \n",
    "    sl_fitness = agent.evaluate_fitness(val_loader)\n",
    "    print(f\"Supervised Learning Correlation: {sl_fitness:.4f}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # PHASE 2: EVOLUTIONARY FINE-TUNING\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 2: EVOLUTIONARY FINE-TUNING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    population_size = 6\n",
    "    num_generations = 5\n",
    "    mutation_rate = 0.03\n",
    "    mutation_scale = 0.02\n",
    "    elitism_count = 2\n",
    "    tournament_size = 3\n",
    "    \n",
    "    # Initialize population\n",
    "    population = []\n",
    "    for i in range(population_size):\n",
    "        new_agent = CorrelationAgent(model_name=model_name)\n",
    "        new_agent.policy_network.load_state_dict(agent.policy_network.state_dict())\n",
    "        \n",
    "        if i > 0:  # Add diversity\n",
    "            params = new_agent.get_parameters(mode=\"head\")\n",
    "            if params.size > 0:\n",
    "                noise_mask = (np.random.rand(len(params)) < mutation_rate)\n",
    "                noise = np.random.normal(0, mutation_scale, noise_mask.sum()).astype(np.float32)\n",
    "                params[noise_mask] += noise\n",
    "                new_agent.set_parameters(params, mode=\"head\")\n",
    "        \n",
    "        population.append(new_agent)\n",
    "    \n",
    "    best_fitness_history = [sl_fitness]\n",
    "    avg_fitness_history = [sl_fitness]\n",
    "    \n",
    "    for generation in range(num_generations):\n",
    "        print(f\"\\nGeneration {generation+1}/{num_generations}\")\n",
    "        fitness_scores = []\n",
    "        \n",
    "        for i, pop_agent in enumerate(population):\n",
    "            fitness = pop_agent.evaluate_fitness(val_loader)\n",
    "            fitness_scores.append(fitness)\n",
    "            print(f\"  Agent {i+1}: {fitness:.4f}\")\n",
    "        \n",
    "        best_fitness = float(np.max(fitness_scores))\n",
    "        avg_fitness = float(np.mean(fitness_scores))\n",
    "        best_fitness_history.append(best_fitness)\n",
    "        avg_fitness_history.append(avg_fitness)\n",
    "        \n",
    "        print(f\"Best: {best_fitness:.4f}, Avg: {avg_fitness:.4f}\")\n",
    "        \n",
    "        # Tournament selection\n",
    "        selected_parents = []\n",
    "        for _ in range(population_size - elitism_count):\n",
    "            tournament_idx = np.random.choice(len(population), tournament_size, replace=False)\n",
    "            tournament_fit = [fitness_scores[i] for i in tournament_idx]\n",
    "            winner = tournament_idx[int(np.argmax(tournament_fit))]\n",
    "            selected_parents.append(population[winner])\n",
    "        \n",
    "        # Elitism\n",
    "        new_population = []\n",
    "        elite_idx = np.argsort(fitness_scores)[-elitism_count:]\n",
    "        for idx in elite_idx:\n",
    "            new_population.append(population[int(idx)])\n",
    "        \n",
    "        # Crossover + Mutation\n",
    "        for _ in range(population_size - elitism_count):\n",
    "            parent1, parent2 = random.sample(selected_parents, 2)\n",
    "            child = CorrelationAgent(model_name=model_name)\n",
    "            \n",
    "            p1 = parent1.get_parameters(mode=\"head\")\n",
    "            p2 = parent2.get_parameters(mode=\"head\")\n",
    "            \n",
    "            if p1.size > 0 and p2.size > 0:\n",
    "                alpha = random.random()\n",
    "                child_params = alpha * p1 + (1 - alpha) * p2\n",
    "                \n",
    "                # Mutation\n",
    "                mut_mask = (np.random.rand(len(child_params)) < mutation_rate)\n",
    "                mut_vals = np.random.normal(0, mutation_scale, mut_mask.sum()).astype(np.float32)\n",
    "                child_params[mut_mask] += mut_vals\n",
    "                \n",
    "                child.set_parameters(child_params, mode=\"head\")\n",
    "            \n",
    "            new_population.append(child)\n",
    "        \n",
    "        population = new_population\n",
    "    \n",
    "    # Select best\n",
    "    final_fitness = [agent.evaluate_fitness(val_loader) for agent in population]\n",
    "    best_idx = int(np.argmax(final_fitness))\n",
    "    best_agent = population[best_idx]\n",
    "    evo_fitness = float(final_fitness[best_idx])\n",
    "    \n",
    "    print(f\"\\nEvolution complete. Best correlation: {evo_fitness:.4f}\")\n",
    "    print(f\"Improvement over SL: {evo_fitness - sl_fitness:.4f}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # PHASE 3: RL BANDIT FINE-TUNING\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 3: RL BANDIT FINE-TUNING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    best_agent.policy_network.train()\n",
    "    rl_optimizer = AdamW(best_agent.policy_network.parameters(), lr=5e-6)\n",
    "    rl_epochs = 2\n",
    "    baseline = 0.0\n",
    "    baseline_beta = 0.9\n",
    "    entropy_coef = 0.01\n",
    "    \n",
    "    for epoch in range(rl_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(best_agent.device) for k, v in batch.items()}\n",
    "            sentiment_pred, alpha_pred = best_agent.forward(batch['input_ids'], batch['attention_mask'])\n",
    "            \n",
    "            # Reward: correlation-based\n",
    "            sent_norm = (sentiment_pred - sentiment_pred.mean()) / (sentiment_pred.std() + 1e-8)\n",
    "            alpha_norm = (batch['alpha'] - batch['alpha'].mean()) / (batch['alpha'].std() + 1e-8)\n",
    "            reward = (sent_norm * alpha_norm)  # Element-wise correlation signal\n",
    "            \n",
    "            # Policy gradient with baseline\n",
    "            baseline = baseline_beta * baseline + (1 - baseline_beta) * reward.mean().item()\n",
    "            adv = reward - baseline\n",
    "            \n",
    "            # Loss (maximize correlation)\n",
    "            loss = -(adv.detach() * sentiment_pred).mean()\n",
    "            \n",
    "            rl_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(best_agent.policy_network.parameters(), 1.0)\n",
    "            rl_optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        val_corr = best_agent.evaluate_fitness(val_loader)\n",
    "        print(f\"RL Epoch {epoch+1}/{rl_epochs}: Loss={epoch_loss/n_batches:.4f}, Val Corr={val_corr:.4f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(best_agent.policy_network.state_dict(), f'{ticker_symbol}_hybrid_final.pt')\n",
    "    print(f\"Final model saved to {ticker_symbol}_hybrid_final.pt\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    test_dataset = CorrelationDataset(test_df, best_agent.tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    test_corr = best_agent.evaluate_fitness(test_loader)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST SET CORRELATION: {test_corr:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(best_fitness_history, label='Best Correlation', marker='o')\n",
    "    plt.plot(avg_fitness_history, label='Avg Correlation', marker='s')\n",
    "    plt.axhline(y=sl_fitness, color='r', linestyle='--', label='SL Baseline')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Correlation (Fitness)')\n",
    "    plt.title(f'{ticker_symbol}: Hybrid Training (SL + Evolution + RL)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{ticker_symbol}_training_progress.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_agent\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# USAGE\n",
    "# ================================================================\n",
    "\n",
    "# ================================================================\n",
    "# USAGE\n",
    "# ================================================================\n",
    "\n",
    "def process_ticker_interactive():\n",
    "    \"\"\"Interactive function to process any ticker\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SENTIMENT-ALPHA CORRELATION TRAINER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get ticker from user\n",
    "    ticker = input(\"\\nEnter ticker symbol (e.g., AAPL, MSFT, GOOGL, TSLA): \").strip().upper()\n",
    "    \n",
    "    if not ticker:\n",
    "        print(\"Error: No ticker provided\")\n",
    "        return None\n",
    "    \n",
    "    # Get data path\n",
    "    data_path = input(\"Enter data directory path (press Enter for current directory): \").strip()\n",
    "    if not data_path:\n",
    "        data_path = './'\n",
    "    \n",
    "    print(f\"\\nProcessing ticker: {ticker}\")\n",
    "    print(f\"Data path: {data_path}\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    required_files = [\n",
    "        f'{ticker}.csv',\n",
    "        f'{ticker}_article_scores.csv',\n",
    "        f'{ticker}_CAPM.csv'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nChecking for required files...\")\n",
    "    missing_files = []\n",
    "    for file in required_files:\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"  ✓ {file}\")\n",
    "        else:\n",
    "            print(f\"  ✗ {file} (NOT FOUND)\")\n",
    "            missing_files.append(file)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"\\nError: Missing files: {missing_files}\")\n",
    "        print(\"Please ensure all three files exist in the specified directory.\")\n",
    "        return None\n",
    "    \n",
    "    # Step 1: Merge files\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"STEP 1: MERGING CSV FILES FOR {ticker}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    merged = merge_csv_files(ticker, base_path=data_path)\n",
    "    \n",
    "    if merged is None:\n",
    "        print(\"Error: Failed to merge files\")\n",
    "        return None\n",
    "    \n",
    "    if len(merged) < 50:\n",
    "        print(f\"Warning: Only {len(merged)} samples found. Recommended minimum: 50\")\n",
    "        proceed = input(\"Continue anyway? (y/n): \").strip().lower()\n",
    "        if proceed != 'y':\n",
    "            return None\n",
    "    \n",
    "    # Step 2: Train\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"STEP 2: TRAINING HYBRID AGENT FOR {ticker}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nThis will run three training phases:\")\n",
    "    print(\"  1. Supervised Learning (3 epochs)\")\n",
    "    print(\"  2. Evolutionary Fine-tuning (5 generations)\")\n",
    "    print(\"  3. RL Bandit Fine-tuning (2 epochs)\")\n",
    "    \n",
    "    proceed = input(\"\\nStart training? (y/n): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        print(\"Training cancelled\")\n",
    "        return None\n",
    "    \n",
    "    agent = train_hybrid_agent(ticker, base_path=data_path)\n",
    "    \n",
    "    # Step 3: Test predictions\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"STEP 3: TESTING PREDICTIONS FOR {ticker}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Default test articles\n",
    "    test_articles = [\n",
    "        f\"{ticker} reports record earnings beating all analyst expectations\",\n",
    "        f\"{ticker} stock drops 5% amid supply chain disruptions and weak guidance\",\n",
    "        f\"{ticker} announces new product line meeting market expectations\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting on sample articles:\")\n",
    "    for i, text in enumerate(test_articles, 1):\n",
    "        sent, alpha = agent.predict(text)\n",
    "        print(f\"\\n{i}. {text}\")\n",
    "        print(f\"   Predicted Sentiment: {sent:+.3f}\")\n",
    "        print(f\"   Predicted Alpha: {alpha:+.3f}\")\n",
    "    \n",
    "    # Allow custom predictions\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Try your own article text (or press Enter to finish)\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    while True:\n",
    "        custom_text = input(\"\\nEnter article text (or press Enter to finish): \").strip()\n",
    "        if not custom_text:\n",
    "            break\n",
    "        \n",
    "        sent, alpha = agent.predict(custom_text)\n",
    "        print(f\"  Predicted Sentiment: {sent:+.3f}\")\n",
    "        print(f\"  Predicted Alpha: {alpha:+.3f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training complete for {ticker}!\")\n",
    "    print(f\"Model saved as: {ticker}_hybrid_final.pt\")\n",
    "    print(f\"Training plot saved as: {ticker}_training_progress.png\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "def process_multiple_tickers():\n",
    "    \"\"\"Process multiple tickers in batch mode\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BATCH PROCESSING MODE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get tickers\n",
    "    tickers_input = input(\"\\nEnter ticker symbols separated by commas (e.g., AAPL,MSFT,GOOGL): \").strip().upper()\n",
    "    tickers = [t.strip() for t in tickers_input.split(',') if t.strip()]\n",
    "    \n",
    "    if not tickers:\n",
    "        print(\"Error: No tickers provided\")\n",
    "        return None\n",
    "    \n",
    "    # Get data path\n",
    "    data_path = input(\"Enter data directory path (press Enter for current directory): \").strip()\n",
    "    if not data_path:\n",
    "        data_path = './'\n",
    "    \n",
    "    print(f\"\\nWill process {len(tickers)} ticker(s): {', '.join(tickers)}\")\n",
    "    proceed = input(\"Continue? (y/n): \").strip().lower()\n",
    "    \n",
    "    if proceed != 'y':\n",
    "        print(\"Cancelled\")\n",
    "        return None\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"PROCESSING TICKER {i}/{len(tickers)}: {ticker}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            # Merge\n",
    "            merged = merge_csv_files(ticker, base_path=data_path)\n",
    "            if merged is None or len(merged) < 50:\n",
    "                print(f\"Skipping {ticker}: Insufficient data\")\n",
    "                continue\n",
    "            \n",
    "            # Train\n",
    "            agent = train_hybrid_agent(ticker, base_path=data_path)\n",
    "            \n",
    "            # Test\n",
    "            test_text = f\"{ticker} reports strong quarterly results\"\n",
    "            sent, alpha = agent.predict(test_text)\n",
    "            \n",
    "            results[ticker] = {\n",
    "                'agent': agent,\n",
    "                'test_sentiment': sent,\n",
    "                'test_alpha': alpha\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n✓ {ticker} completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error processing {ticker}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BATCH PROCESSING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nSuccessfully processed {len(results)}/{len(tickers)} tickers:\\n\")\n",
    "        for ticker in results:\n",
    "            print(f\"  ✓ {ticker}\")\n",
    "            print(f\"      Model: {ticker}_hybrid_final.pt\")\n",
    "            print(f\"      Plot: {ticker}_training_progress.png\")\n",
    "    else:\n",
    "        print(\"\\nNo tickers were successfully processed\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# USAGE\n",
    "# ================================================================\n",
    "\n",
    "def process_ticker_interactive():\n",
    "    \"\"\"Interactive function to process any ticker\"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SENTIMENT-ALPHA CORRELATION TRAINER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get ticker from user\n",
    "    ticker = input(\"\\nEnter ticker symbol (e.g., AAPL, MSFT, GOOGL, TSLA): \").strip().upper()\n",
    "    \n",
    "    if not ticker:\n",
    "        print(\"Error: No ticker provided\")\n",
    "        return None\n",
    "    \n",
    "    # Get data path\n",
    "    data_path = input(\"Enter data directory path (press Enter for current directory): \").strip()\n",
    "    if not data_path:\n",
    "        data_path = './'\n",
    "    \n",
    "    print(f\"\\nProcessing ticker: {ticker}\")\n",
    "    print(f\"Data path: {data_path}\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    required_files = [\n",
    "        f'{ticker}.csv',\n",
    "        f'{ticker}_article_scores.csv',\n",
    "        f'{ticker}_CAPM.csv'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nChecking for required files...\")\n",
    "    missing_files = []\n",
    "    for file in required_files:\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"  ✓ {file}\")\n",
    "        else:\n",
    "            print(f\"  ✗ {file} (NOT FOUND)\")\n",
    "            missing_files.append(file)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"\\nError: Missing files: {missing_files}\")\n",
    "        print(\"Please ensure all three files exist in the specified directory.\")\n",
    "        return None\n",
    "    \n",
    "    # Step 1: Merge files\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"STEP 1: MERGING CSV FILES FOR {ticker}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    merged = merge_csv_files(ticker, base_path=data_path)\n",
    "    \n",
    "    if merged is None:\n",
    "        print(\"Error: Failed to merge files\")\n",
    "        return None\n",
    "    \n",
    "    if len(merged) < 50:\n",
    "        print(f\"Warning: Only {len(merged)} samples found. Recommended minimum: 50\")\n",
    "        proceed = input(\"Continue anyway? (y/n): \").strip().lower()\n",
    "        if proceed != 'y':\n",
    "            return None\n",
    "    \n",
    "    # Step 2: Train\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"STEP 2: TRAINING HYBRID AGENT FOR {ticker}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nThis will run three training phases:\")\n",
    "    print(\"  1. Supervised Learning (3 epochs)\")\n",
    "    print(\"  2. Evolutionary Fine-tuning (5 generations)\")\n",
    "    print(\"  3. RL Bandit Fine-tuning (2 epochs)\")\n",
    "    \n",
    "    proceed = input(\"\\nStart training? (y/n): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        print(\"Training cancelled\")\n",
    "        return None\n",
    "    \n",
    "    agent = train_hybrid_agent(ticker, base_path=data_path)\n",
    "    \n",
    "    # Step 3: Test predictions\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"STEP 3: TESTING PREDICTIONS FOR {ticker}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Default test articles\n",
    "    test_articles = [\n",
    "        f\"{ticker} reports record earnings beating all analyst expectations\",\n",
    "        f\"{ticker} stock drops 5% amid supply chain disruptions and weak guidance\",\n",
    "        f\"{ticker} announces new product line meeting market expectations\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting on sample articles:\")\n",
    "    for i, text in enumerate(test_articles, 1):\n",
    "        sent, alpha = agent.predict(text)\n",
    "        print(f\"\\n{i}. {text}\")\n",
    "        print(f\"   Predicted Sentiment: {sent:+.3f}\")\n",
    "        print(f\"   Predicted Alpha: {alpha:+.3f}\")\n",
    "    \n",
    "    # Allow custom predictions\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Try your own article text (or press Enter to finish)\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    while True:\n",
    "        custom_text = input(\"\\nEnter article text (or press Enter to finish): \").strip()\n",
    "        if not custom_text:\n",
    "            break\n",
    "        \n",
    "        sent, alpha = agent.predict(custom_text)\n",
    "        print(f\"  Predicted Sentiment: {sent:+.3f}\")\n",
    "        print(f\"  Predicted Alpha: {alpha:+.3f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training complete for {ticker}!\")\n",
    "    print(f\"Model saved as: {ticker}_hybrid_final.pt\")\n",
    "    print(f\"Training plot saved as: {ticker}_training_progress.png\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "def process_multiple_tickers():\n",
    "    \"\"\"Process multiple tickers in batch mode\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BATCH PROCESSING MODE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get tickers\n",
    "    tickers_input = input(\"\\nEnter ticker symbols separated by commas (e.g., AAPL,MSFT,GOOGL): \").strip().upper()\n",
    "    tickers = [t.strip() for t in tickers_input.split(',') if t.strip()]\n",
    "    \n",
    "    if not tickers:\n",
    "        print(\"Error: No tickers provided\")\n",
    "        return None\n",
    "    \n",
    "    # Get data path\n",
    "    data_path = input(\"Enter data directory path (press Enter for current directory): \").strip()\n",
    "    if not data_path:\n",
    "        data_path = './'\n",
    "    \n",
    "    print(f\"\\nWill process {len(tickers)} ticker(s): {', '.join(tickers)}\")\n",
    "    proceed = input(\"Continue? (y/n): \").strip().lower()\n",
    "    \n",
    "    if proceed != 'y':\n",
    "        print(\"Cancelled\")\n",
    "        return None\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"PROCESSING TICKER {i}/{len(tickers)}: {ticker}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            # Merge\n",
    "            merged = merge_csv_files(ticker, base_path=data_path)\n",
    "            if merged is None or len(merged) < 50:\n",
    "                print(f\"Skipping {ticker}: Insufficient data\")\n",
    "                continue\n",
    "            \n",
    "            # Train\n",
    "            agent = train_hybrid_agent(ticker, base_path=data_path)\n",
    "            \n",
    "            # Test\n",
    "            test_text = f\"{ticker} reports strong quarterly results\"\n",
    "            sent, alpha = agent.predict(test_text)\n",
    "            \n",
    "            results[ticker] = {\n",
    "                'agent': agent,\n",
    "                'test_sentiment': sent,\n",
    "                'test_alpha': alpha\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n✓ {ticker} completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error processing {ticker}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BATCH PROCESSING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nSuccessfully processed {len(results)}/{len(tickers)} tickers:\\n\")\n",
    "        for ticker in results:\n",
    "            print(f\"  ✓ {ticker}\")\n",
    "            print(f\"      Model: {ticker}_hybrid_final.pt\")\n",
    "            print(f\"      Plot: {ticker}_training_progress.png\")\n",
    "    else:\n",
    "        print(\"\\nNo tickers were successfully processed\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# DIRECT FUNCTIONS FOR JUPYTER/VS CODE\n",
    "# ================================================================\n",
    "\n",
    "def train_single_ticker(ticker, data_path='./'):\n",
    "    \"\"\"\n",
    "    Direct function to train a single ticker without interactive prompts.\n",
    "    Perfect for Jupyter notebooks.\n",
    "    \n",
    "    Args:\n",
    "        ticker: Stock ticker symbol (e.g., 'AAPL', 'MSFT')\n",
    "        data_path: Path to directory containing CSV files\n",
    "    \n",
    "    Returns:\n",
    "        trained agent\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING AGENT FOR {ticker}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Check files\n",
    "    required_files = [\n",
    "        f'{ticker}.csv',\n",
    "        f'{ticker}_article_scores.csv',\n",
    "        f'{ticker}_CAPM.csv'\n",
    "    ]\n",
    "    \n",
    "    print(\"Checking files...\")\n",
    "    for file in required_files:\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        exists = \"✓\" if os.path.exists(file_path) else \"✗\"\n",
    "        print(f\"  {exists} {file}\")\n",
    "    \n",
    "    # Merge\n",
    "    merged = merge_csv_files(ticker, base_path=data_path)\n",
    "    if merged is None or len(merged) < 50:\n",
    "        print(f\"Error: Insufficient data for {ticker}\")\n",
    "        return None\n",
    "    \n",
    "    # Train\n",
    "    agent = train_hybrid_agent(ticker, base_path=data_path)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ Training complete for {ticker}!\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "def train_multiple_tickers(tickers, data_path='./'):\n",
    "    \"\"\"\n",
    "    Train multiple tickers without interactive prompts.\n",
    "    Perfect for Jupyter notebooks.\n",
    "    \n",
    "    Args:\n",
    "        tickers: List of ticker symbols (e.g., ['AAPL', 'MSFT', 'GOOGL'])\n",
    "        data_path: Path to directory containing CSV files\n",
    "    \n",
    "    Returns:\n",
    "        dictionary of trained agents\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BATCH TRAINING: {len(tickers)} TICKERS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(f\"\\n[{i}/{len(tickers)}] Processing {ticker}...\")\n",
    "        \n",
    "        try:\n",
    "            agent = train_single_ticker(ticker, data_path)\n",
    "            if agent is not None:\n",
    "                results[ticker] = agent\n",
    "                print(f\"✓ {ticker} completed\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {ticker} failed: {e}\\n\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY: {len(results)}/{len(tickers)} successful\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for ticker in results:\n",
    "        print(f\"  ✓ {ticker}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def test_agent(agent, ticker, custom_texts=None):\n",
    "    \"\"\"\n",
    "    Test a trained agent with sample or custom texts.\n",
    "    Perfect for Jupyter notebooks.\n",
    "    \n",
    "    Args:\n",
    "        agent: Trained CorrelationAgent\n",
    "        ticker: Ticker symbol (for generating default texts)\n",
    "        custom_texts: Optional list of custom article texts to test\n",
    "    \"\"\"\n",
    "    if custom_texts is None:\n",
    "        custom_texts = [\n",
    "            f\"{ticker} reports record earnings beating all analyst expectations\",\n",
    "            f\"{ticker} stock drops 5% amid supply chain disruptions and weak guidance\",\n",
    "            f\"{ticker} announces new product line meeting market expectations\"\n",
    "        ]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING AGENT FOR {ticker}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for i, text in enumerate(custom_texts, 1):\n",
    "        sent, alpha = agent.predict(text)\n",
    "        results.append({'text': text, 'sentiment': sent, 'alpha': alpha})\n",
    "        print(f\"{i}. {text}\")\n",
    "        print(f\"   Sentiment: {sent:+.3f} | Alpha: {alpha:+.3f}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# MAIN ENTRY POINT\n",
    "# ================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if running in Jupyter/IPython\n",
    "    try:\n",
    "        get_ipython()\n",
    "        IN_NOTEBOOK = True\n",
    "    except NameError:\n",
    "        IN_NOTEBOOK = False\n",
    "    \n",
    "    if IN_NOTEBOOK:\n",
    "        # Running in Jupyter - provide simple example\n",
    "        print(\"=\"*70)\n",
    "        print(\"  RUNNING IN JUPYTER/VS CODE NOTEBOOK\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nUse these functions directly in cells:\")\n",
    "        print(\"\\n# Train single ticker:\")\n",
    "        print(\"agent = train_single_ticker('AAPL', data_path='./')\")\n",
    "        print(\"\\n# Train multiple tickers:\")\n",
    "        print(\"agents = train_multiple_tickers(['AAPL', 'MSFT', 'GOOGL'], data_path='./')\")\n",
    "        print(\"\\n# Test predictions:\")\n",
    "        print(\"test_agent(agent, 'AAPL')\")\n",
    "        print(\"\\n# Custom test:\")\n",
    "        print(\"custom_texts = ['Your custom article text here']\")\n",
    "        print(\"test_agent(agent, 'AAPL', custom_texts=custom_texts)\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "    else:\n",
    "        # Running as script - use interactive menu\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"  SENTIMENT-ALPHA CORRELATION TRAINER\")\n",
    "        print(\"  Hybrid Training: Supervised Learning + Evolution + RL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nSelect mode:\")\n",
    "        print(\"  1. Single ticker (interactive)\")\n",
    "        print(\"  2. Multiple tickers (batch)\")\n",
    "        print(\"  3. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter choice (1-3): \").strip()\n",
    "        \n",
    "        if choice == '1':\n",
    "            agent = process_ticker_interactive()\n",
    "            \n",
    "        elif choice == '2':\n",
    "            results = process_multiple_tickers()\n",
    "            \n",
    "        elif choice == '3':\n",
    "            print(\"Exiting...\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid choice\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MQS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
