{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install required libraries if not already done\n",
    "!pip install transformers datasets scikit-learn torch pandas matplotlib seaborn jupyter kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28f1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"ankurzing/sentiment-analysis-for-financial-news\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Define file path\n",
    "file_path = os.path.join(path, \"all-data.csv\")\n",
    "\n",
    "# Read CSV with no header and proper encoding\n",
    "df = pd.read_csv(file_path, engine='python', sep=',', header=None, encoding='latin-1')\n",
    "\n",
    "# Check the first few rows to understand column order\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Assign column names manually based on inspection\n",
    "# Based on public dataset info, it's usually [sentiment, text]\n",
    "df.columns = ['label', 'text']\n",
    "\n",
    "# Now filter valid labels\n",
    "valid_labels = {'positive', 'neutral', 'negative'}\n",
    "df['label'] = df['label'].str.strip()\n",
    "df = df[df['label'].isin(valid_labels)]\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optional: print shapes\n",
    "print(\"\\nTrain shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "model_name = \"yiyanghkust/finbert-tone\"  # FinBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Convert pandas DataFrames to Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Define your label mappings correctly (only the valid ones)\n",
    "labels = ['positive', 'neutral', 'negative']\n",
    "\n",
    "# Create mappings\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Map string labels to integers within the tokenization function\n",
    "    # Ensure examples['label'] is a string before mapping\n",
    "    label_str = examples[\"label\"]\n",
    "    examples[\"labels\"] = label2id[label_str]\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=False)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "# Remove the original 'label' column as we now have 'labels'\n",
    "tokenized_train = tokenized_train.remove_columns([\"label\"])\n",
    "tokenized_test = tokenized_test.remove_columns([\"label\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels), # Use len(labels) to be dynamic\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"finbert-finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=18,\n",
    "    per_device_eval_batch_size=18,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"linear\",  # ðŸ‘ˆ corrected scheduler\n",
    "    warmup_steps=500,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,    # â† here\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"finbert-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"finbert-finetuned-final\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Load and Prep Kaggle Dataset (Financial PhraseBank)\n",
    "# ---------------------------------------------------------\n",
    "path = kagglehub.dataset_download(\"ankurzing/sentiment-analysis-for-financial-news\")\n",
    "kaggle_file_path = os.path.join(path, \"all-data.csv\")\n",
    "\n",
    "# Read Kaggle CSV\n",
    "df_kaggle = pd.read_csv(kaggle_file_path, engine='python', sep=',', header=None, encoding='latin-1')\n",
    "df_kaggle.columns = ['label', 'text'] # Rename to standard columns\n",
    "\n",
    "# Filter only valid labels\n",
    "df_kaggle['label'] = df_kaggle['label'].str.strip()\n",
    "df_kaggle = df_kaggle[df_kaggle['label'].isin(['positive', 'neutral', 'negative'])]\n",
    "\n",
    "print(f\"Kaggle rows: {len(df_kaggle)}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Load and Prep FiQA Dataset\n",
    "# ---------------------------------------------------------\n",
    "fiqa_data = load_dataset(\"TheFinAI/fiqa-sentiment-classification\")\n",
    "df_fiqa = fiqa_data[\"train\"].to_pandas()  # Using 'train' split for more data\n",
    "\n",
    "# Define conversion logic (Modified to return STRINGS to match Kaggle)\n",
    "def score_to_label_str(score):\n",
    "    # Using 0.0 as cutoff as requested, but returning strings\n",
    "    if score > 0.0:\n",
    "        return \"positive\"\n",
    "    elif score < -0.0:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Apply conversion\n",
    "df_fiqa['label'] = df_fiqa['score'].apply(score_to_label_str)\n",
    "df_fiqa = df_fiqa.rename(columns={'sentence': 'text'})\n",
    "\n",
    "# Keep only the columns we need\n",
    "df_fiqa = df_fiqa[['label', 'text']]\n",
    "\n",
    "print(f\"FiQA rows: {len(df_fiqa)}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. MERGE AND SHUFFLE (The Critical Step)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Concatenate both dataframes\n",
    "combined_df = pd.concat([df_kaggle, df_fiqa], ignore_index=True)\n",
    "\n",
    "# Shuffle the rows randomly (frac=1 means sample 100% of rows)\n",
    "# random_state=42 ensures reproducible shuffling\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total Combined rows: {len(combined_df)}\")\n",
    "print(\"First 5 rows of mixed data:\")\n",
    "print(combined_df.head())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Resume Your Standard Pipeline\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Split data (now containing a mix of both sources)\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(\"\\nReady for tokenization and training!\")\n",
    "\n",
    "# 1. Define Labels\n",
    "labels = ['positive', 'neutral', 'negative']\n",
    "model_name = \"yiyanghkust/finbert-tone\"  # FinBERT\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Create Mappings\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)\n",
    "\n",
    "# 3. Tokenize with the new mappings\n",
    "def tokenize_function(examples):\n",
    "    label_str = examples[\"label\"]\n",
    "    examples[\"labels\"] = label2id[label_str]\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=False)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "# 4. Clean Columns\n",
    "tokenized_train = tokenized_train.remove_columns([\"label\"])\n",
    "tokenized_test = tokenized_test.remove_columns([\"label\"])\n",
    "\n",
    "# 5. Load Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 6. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"finbert-combined-finetuned\", # Changed name to reflect combined data\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=18,\n",
    "    per_device_eval_batch_size=18,\n",
    "    num_train_epochs=3, # Reduced to 3 as 5 might overfit with more data\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=500,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# 7. Initialize Trainer\n",
    "# NOTE: The 'compute_metrics' function here typically uses np.argmax(logits).\n",
    "# This works because the highest logit ALWAYS corresponds to the highest Softmax probability.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")\n",
    "\n",
    "# 8. Train and Save\n",
    "trainer.train()\n",
    "model.save_pretrained(\"finbert-combined-final\")\n",
    "tokenizer.save_pretrained(\"finbert-combined-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip & download your saved model folder in Colab\n",
    "\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Define the correct directory name\n",
    "directory_to_zip = \"finbert-base-model\"\n",
    "\n",
    "# 1. Make a .zip archive of the saved model directory\n",
    "shutil.make_archive(\n",
    "    base_name=directory_to_zip,  # The name for the output .zip file\n",
    "    format=\"zip\",\n",
    "    root_dir=directory_to_zip    # The folder you want to zip\n",
    ")\n",
    "\n",
    "# 2. Download it to your local machine\n",
    "print(f\"âœ… Archiving complete. Downloading {directory_to_zip}.zip...\")\n",
    "files.download(f\"{directory_to_zip}.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mqs (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
